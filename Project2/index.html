<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS280A · Project 2 — Fun with Filters & Frequencies</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --max: 980px; }
    * { box-sizing: border-box; }
    body { margin: 24px; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif; line-height: 1.6; background:#fafafa;}
    main, header, footer { max-width: var(--max); margin: 0 auto; }
    h1, h2, h3 { line-height: 1.25; }
    .muted { color:#666; font-size: 14px; }
    .pair { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 14px; margin:14px 0;}
    figure { margin: 0; border: 1px solid #eaeaea; border-radius: 10px; padding: 10px; background: #fff; }
    figcaption { margin-top: 6px; font-size: 14px; color: #555; }
    img { max-width: 100%; height: auto; display: block; background: #f5f5f5; border-radius: 6px; }
    .answer { border-left: 4px solid #2f6feb; background: #f0f6ff; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    .prompt { border-left: 4px solid #999; background: #f7f7f7; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    nav a { text-decoration: none; color:#2f6feb; }
    hr { border: none; height: 1px; background: #eee; margin: 28px 0; }
    .toc a { color:#2f6feb; text-decoration:none; }
    .row { display:flex; gap:14px; flex-wrap:wrap; }
    .row figure { flex:1 1 300px; }
    .tight figure { padding:6px; }
    .wide figure img { width:100%; }
  </style>
</head>
<body>
  <header>
    <p class="muted"><a href="../index.html">← Back to Home</a></p>
    <h1>Project 2 — Fun with Filters & Frequencies</h1>
    <p class="muted">CS280A · Wentio Li · Sept 2025</p>
  </header>

  <main>
    <!-- TOC -->
    <section class="toc">
      <p>
        <a href="#overview"> Overview</a> ·
        <a href="#p1">Part 1: Fun with Filters</a> ·
        <a href="#p2">Part 2: Applications</a> ·
      </p>
    </section>

    <hr />

    <!-- ===================== PART 1 ===================== -->

    <!-- 1. Project Overview -->
    <section id="overview">
      <h2>Project Overview</h2>
      <div class="prompt">
        <p>
          This project explores fundamental concepts in image filtering and frequency-based techniques, serving as an introduction to computational photography and computer vision. In Part 1, we build intuition about 2D convolutions and edge detection, starting from finite difference operators, progressing to Gaussian smoothing, and finally deriving the derivative of Gaussian (DoG) filters. We also visualize gradient orientations using color mappings to better understand image structure.
        </p>
        <p>
          In Part 2, we shift focus to frequency manipulations and creative applications. We implement image sharpening with unsharp masking, construct hybrid images that reveal different content at varying viewing distances, and develop Gaussian and Laplacian stacks as the foundation for multi-resolution blending. The project culminates in seamlessly blending images—such as the classic "Oraple"—and extending the idea with irregular masks and color enhancements.  </p>     
      </div>

    <hr />

    
    <section id="p1">
      <h2>Part 1: Fun with Filters</h2>

      <!-- 1.1 -->
      <section id="p11">
        <h3>1.1 Convolutions from Scratch</h3>
        <div class="prompt">
          <p>
            I implemented 2D convolution from scratch with NumPy, first using a naive
            four-loop version and then a more efficient two-loop version that computes
            patch-wise inner products. Both implementations flip the kernel and apply zero padding so the output image matches the input
            size. 
          </p>
          <p>    
            For verification, I compared against
            <code>scipy.signal.convolve2d</code> with the same padding, and confirmed
            that the results match up to numerical precision while the two-loop version
            runs faster than the four-loop baseline. The experiments include applying a
            9×9 box filter to a grayscale selfie, and convolving with finite-difference
            kernels (<code>Dx=[1,0,-1]</code>, <code>Dy=Dx<sup>T</sup></code>) to
            visualize horizontal and vertical changes.
          </p>
 
          <p><strong>Key code snippet:</strong> </p>
        
          <pre><code class="language-python">
        def zero_pad(img, kH, kW):
            py, px = kH // 2, kW // 2
            return np.pad(img, ((py, py), (px, px)), mode="constant", constant_values=0.0)
        
        def conv2d_2loops(img, kernel):
            H, W = img.shape
            kH, kW = kernel.shape
            k = np.flipud(np.fliplr(kernel))      # flip for convolution
            padded = zero_pad(img, kH, kW)
            out = np.zeros((H, W), dtype=np.float32)
            for i in range(H):
                for j in range(W):
                    patch = padded[i:i+kH, j:j+kW]
                    out[i, j] = np.sum(patch * k) # inner product over the patch
            return out
        
        def box_filter(k=9):
            assert k % 2 == 1
            return np.ones((k, k), dtype=np.float32) / (k * k)
        
        Dx = np.array([[1, 0, -1]], dtype=np.float32); Dy = Dx.T</code></pre>
        
          <p><strong>Inputs &amp; operations.</strong> The selfie is read as grayscale and scaled to [0,1]. I convolve it with a 9×9 box filter, and also with <code>Dx</code>/<code>Dy</code> to visualize directional changes. For comparison I run SciPy with identical padding and show the MAE to demonstrate equivalence.</p>
        </div>

        <figure>
            <img src="./data/self_1.jpeg" alt="selfie" width="400" />
            <figcaption>
              Selfie at Monterey Bay Aquarium, Original
            </figcaption>
        </figure>
        <div class="pair">
          <figure><img src="./result/1.1/self_1_box9_4loops.png" alt="box 4 loops"><figcaption>4 loops, time=97.44s</figcaption></figure>
          <figure><img src="./result/1.1/self_1_box9_2loops.png" alt="box 2 loops"><figcaption>2 loops, time=13.43s</figcaption></figure>
          <figure><img src="./result/1.1/self_1_box9_scipy.png" alt="scipy ref"><figcaption>scipy.signal.convolve2d, time=0.38s</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/1.1/self_1_Dx.png" alt="Dx"><figcaption>Finite diff Dx</figcaption></figure>
          <figure><img src="./result/1.1/self_1_Dy.png" alt="Dy"><figcaption>Finite diff Dy</figcaption></figure>
        </div>
        <div class="metrics">
          <pre>
          MAE (4-loops vs 2-loops): <code>8.889e-08</code>. 
          MAE (SciPy vs 2-loops): <code>8.998e-08</code>.

          </pre>
        </div>
        <div class="answer">

          <p>
            The three implementations produce almost identical results, confirming correct convolution (kernel flip, zero padding, same-size output). Runtimes: 4-loops = <code>97.44s</code>, 2-loops = <code>13.43s</code>, SciPy = <code>0.383s</code>. The 2-loop version is much faster than the 4-loop baseline, and SciPy is faster than both due to optimized backends.
          </p>
          <p>
            Boundaries use zero padding (equivalent to <code>boundary="fill", fillvalue=0</code> in SciPy), so the output size matches the input. Zero padding can darken edges for box filtering and create one-sided responses for <code>Dx/Dy</code>. This behavior is expected and kept here to match the specification.
          </p>
        </div>

      </section>

      <!-- 1.2 -->
      <section id="p12">
        <h3>1.2 Finite Difference Operator</h3>
        <div class="prompt">
          <p>
            For the cameraman image, I applied finite difference operators <code>Dx=[1,0,-1]</code> and <code>Dy=Dx<sup>T</sup></code> using <code>scipy.signal.convolve2d</code> with zero padding. This produces the partial derivatives in the x and y directions, which highlight horizontal and vertical intensity changes. From these two responses, I computed the gradient magnitude image as <code>sqrt(Gx^2 + Gy^2)</code> to capture edge strength independent of direction.
          </p>
          <p>
            To convert the gradient magnitude into a binary edge map, I performed a percentile-based threshold sweep (70–95%) and qualitatively selected a level that balances noise suppression with preserving important edges. Lower thresholds retain more detail but also amplify noise, while higher thresholds reduce noise at the cost of missing weaker edges. The final choice reflects this tradeoff and provides a clean yet informative edge visualization.
          </p>
        </div>
        <figure>
            <img src="./data/cameraman.png" alt="cm" width="400" />
            <figcaption>
              Cameram Man, Original
            </figcaption>
        </figure>
        <div class="pair">
          <figure><img src="./result/1.2/cameraman_Gx.png" alt="Gx"><figcaption>Gx</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_Gy.png" alt="Gy"><figcaption>Gy</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_gradmag.png" alt="gradmag"><figcaption>Gradient magnitude</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/1.2/cameraman_edge_p85.png" alt="edge p85"><figcaption>Edge (threshold ~p85)</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_edge_p90.png" alt="edge p90"><figcaption>Edge (threshold ~p90)</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_edge_p95.png" alt="edge p95"><figcaption>Edge (threshold ~p95)</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            The partial derivatives clearly highlight horizontal and vertical changes, and the gradient magnitude combines them into a strong overall edge response. To create a binary edge map, I tested percentile thresholds from 85% to 95%. At 85% the edges are dense but the grass region shows many noisy dots. At 95% most background noise is suppressed, but some weaker object contours begin to disappear. The 90% threshold offers the best balance: it removes most of the grass noise while keeping the main outlines of the cameraman, tripod, and background structures intact. Therefore, the edge map at 90% is selected as the final result.
          </p>
        </div>
      </section>

      <!-- 1.3 -->
      <section id="p13">
        <h3>1.3 Derivative of Gaussian (DoG) </h3>
        <div class="prompt">
          <p>
            <strong>Method 1, Gaussian Smoothing: </strong>To reduce the noise seen with plain finite differences, I first smoothed the cameraman image with a Gaussian filter created from <code>cv2.getGaussianKernel</code> (size 7, sigma 1.5). After blurring, the gradients in x and y were computed with <code>Dx</code> and <code>Dy</code>, and the resulting magnitude image shows clearer object boundaries with much less background noise compared to Part&nbsp;1.2.
          </p> 
          <p>
            <strong>Method 2, DoG Filters:</strong> For comparison, I also built derivative-of-Gaussian (DoG) filters by convolving the Gaussian kernel directly with <code>Dx</code> and <code>Dy</code>. Applying these DoG filters to the original image produces results that closely match the “blur then differentiate” approach, confirming their equivalence while highlighting the advantage of a single convolution step.
          </p>
        </div>
        <div class="pair">
          <figure>
            <img src="./result/1.3/cameraman_blurred.png" alt="Gaussian blurred">
            <figcaption>Gaussian-blurred image (ksize=7, σ=1.5)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_Gx.png" alt="Gaussian blur then Dx">
            <figcaption>Gaussian-blur → Dx (Gx)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_Gy.png" alt="Gaussian blur then Dy">
            <figcaption>Gaussian-blur → Dy (Gy)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_gradmag.png" alt="Gaussian blur grad mag">
            <figcaption>Gradient magnitude after Gaussian-blur → (Dx,Dy)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/1.3/cameraman_DoGx.png" alt="DoGx response">
            <figcaption>Convolve(I, DoGx) (x-response)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_DoGy.png" alt="DoGy response">
            <figcaption>Convolve(I, DoGy) (y-response)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_DoG_gradmag.png" alt="DoG grad mag">
            <figcaption>Gradient magnitude using DoG filters</figcaption>
          </figure>
        </div>

        <div class="answer">
          <p>
            The Gaussian-blur → derivative results confirm that smoothing reduces the noise seen in Part&nbsp;1.2. The gradient magnitude shows cleaner object boundaries, especially around the cameraman and tripod, while the grass region is much less noisy compared to the plain finite difference method.
          </p>
          <p>
            The DoG responses demonstrate the same effect in a more efficient way. By convolving the Gaussian kernel with <code>Dx</code> and <code>Dy</code> in advance, the DoG filters combine smoothing and differentiation into a single convolution. The resulting x- and y-responses, as well as the gradient magnitude, are visually almost identical to the blur→derivative pipeline. This verifies the mathematical equivalence of the two approaches and highlights the benefit of using DoG filters for edge detection with reduced noise.
          </p>
        </div>

      </section>
    </section>

     <section id="p14">
        <h3>1.4 Bells &amp; Whistles: HSV color space</h3>
       <div class="prompt">
        <p>
          In this extension, I computed gradient orientations using the DoG-based derivatives of the cameraman image. The orientation is given by <code>θ = arctan2(Gy, Gx)</code>, which I normalized to the range [0, π) because edge direction is π-periodic (a gradient pointing left is equivalent to one pointing right). 
          To visualize orientations, I mapped this normalized angle to hue in HSV color space, while using gradient magnitude for both saturation and value. 
          A percentile threshold on the magnitude suppresses weak gradients so that only meaningful edges are colored. 
          The resulting HSV image clearly encodes edge direction through hue variation, while brightness corresponds to edge strength.
        </p>
      </div>
        <div class="pair">
          <figure><img src="./result/1.4/cameraman_orientation_hsv.png" alt="orientation HSV"><figcaption>gradient orientation (HSV)</figcaption></figure>
        </div>
       <div class="answer">
        <p>
          The HSV visualization successfully encodes orientation as color, with different hues representing different edge directions. 
          Strong contours around the cameraman, tripod, and background structures show clear color transitions, while flat regions remain dark due to low gradient magnitude. 
          This confirms that combining orientation with magnitude in HSV space provides an intuitive way to observe both edge strength and direction in a single image.
        </p>
      </div>

      </section>
    </section>

    <hr />

    <!-- ===================== PART 2 ===================== -->
    <section id="p2">
      <h2>Part 2: Applications</h2>

      <!-- 2.1 Sharpening -->
      <section id="p21">
        <h3>2.1 Image “Sharpening” (Unsharp Mask)</h3>
        <div class="prompt">
          <p>
            In this part, I implemented the unsharp mask filter to enhance image sharpness by boosting high-frequency components. 
            A Gaussian filter is first applied as a low-pass operator to capture the smooth background (low frequencies). 
            Subtracting this blurred version from the original gives the high-pass component, which highlights edges and fine details. 
            The sharpened result is then obtained as <code>Isharp = I + α · (I − L)</code>, where <code>α</code> controls how strongly the high frequencies are added back. 
          </p>
          <p>
            For evaluation, I applied this method to the Taj Mahal image and produced the blurred, high-pass, and sharpened versions. 
            Multiple values of <code>α</code> (0.5, 1.0, 1.5) are shown to demonstrate the effect of varying the sharpening strength. 
            As a second case, I used my selfie image: the original was blurred with a Gaussian filter and then sharpened again. 
            This setup allows a direct comparison between the original and reconstructed sharpness, and PSNR values are reported to quantify the recovery quality. 
          </p>
        </div>

        <div class="pair">
          <figure>
            <img src="./result/2.1/taj_original.png" alt="taj original">
            <figcaption>Taj — original</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_lowpass.png" alt="taj low-pass">
            <figcaption>Taj — low-pass (Gaussian blur)</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_highpass.png" alt="taj high-pass">
            <figcaption>Taj — high-pass (original − low-pass)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/taj_sharp_a0.5.png" alt="taj sharp a0.5">
            <figcaption>Taj — sharpen α=0.5</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_sharp_a1.0.png" alt="taj sharp a1.0">
            <figcaption>Taj — sharpen α=1.0</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_sharp_a1.5.png" alt="taj sharp a1.5">
            <figcaption>Taj — sharpen α=1.5</figcaption>
          </figure>
        </div>


      <div class="answer">
        <p>
          The Taj Mahal example clearly shows how unsharp masking separates low and high frequency components. 
          The low-pass version smooths out fine details, while the high-pass highlights edges and textures. 
          Adding scaled high frequencies back to the original produces sharper images, with α=1.0 giving a natural enhancement and α=1.5 starting to introduce more aggressive contrast on edges. 
        </p>
      </div>
      </section>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/self_original.png" alt="self original">
            <figcaption>Selfie — original</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sigma1.8.png" alt="self blur">
            <figcaption>Selfie — blurred (σ=1.8)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/self_blur_sharp_a0.5_s1.8.png" alt="self sharp a0.5">
            <figcaption>Selfie — sharpen α=0.5</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sharp_a1.0_s1.8.png" alt="self sharp a1.0">
            <figcaption>Selfie — sharpen α=1.0</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sharp_a1.5_s1.8.png" alt="self sharp a1.5">
            <figcaption>Selfie — sharpen α=1.5</figcaption>
          </figure>
        </div>
           <div class="metrics">
          <pre>
        PSNR (reference = self_original.png)
        blur (σ=1.8): PSNR = 36.182 dB
        sharpen (α=0.5, σ=1.8): PSNR = 37.522 dB
        sharpen (α=1.0, σ=1.8): PSNR = 38.742 dB
        sharpen (α=1.5, σ=1.8): PSNR = 39.576 dB
          </pre>
        </div>
        

      <div class="answer">
        <p>
          On the selfie test, blurring reduced PSNR to 36.18 dB relative to the original. 
          Sharpening improved the score to 37.52 dB (α=0.5), 38.74 dB (α=1.0), and 39.58 dB (α=1.5). 
          Visually, α=1.5 recovers most details with strongest edges. 
          These results confirm that unsharp masking effectively restores detail by amplifying high frequencies, and the tradeoff is controlled by the sharpening strength α. 
        </p>
      </div>
      </section>

      <!-- 2.2 Hybrids (280A color experiments) -->
      <section id="p22">
        <h3>2.2 Hybrid Images (280A color experiments included)</h3>
      <div class="prompt">
        <p>
          In this part, I created hybrid images by combining the low-frequency content of one image with the high-frequency content of another. 
          The idea is that when viewed up close, the high frequencies dominate perception, but from far away, only the smooth low frequencies remain visible. 
          This effect allows the same static image to appear as two different subjects depending on viewing distance. 
        </p>
        <p>
          The first example uses the provided Derek and Nutmeg images. 
          I aligned them, applied a Gaussian low-pass filter to Derek, and extracted high frequencies from Nutmeg. 
          After blending, the hybrid shows Nutmeg’s details at close range but transitions to Derek’s face when viewed from a distance. 
          To illustrate the process, I also performed Fourier analysis to compare the input images, their filtered versions, and the final hybrid in the frequency domain. 
          In addition, I experimented with different color strategies, such as keeping low-pass color with high-pass grayscale, or vice versa, to study which approach gives the clearest perceptual separation. 
        </p>
      </div>

        <h4>Full pipeline (Derek + Nutmeg)</h4>
          <!-- Inputs & mask (for context) -->
          <div class="pair">
            <figure>
              <img src="./result/2.2/fft_input_low_Derek.png" alt="FFT input Derek (low)">
              <figcaption>FFT — input (Derek, low-frequency source)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/fft_input_high_Nutmeg.png" alt="FFT input Nutmeg (high)">
              <figcaption>FFT — input (Nutmeg, high-frequency source)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/roi_mask_soft.png" alt="soft mask">
              <figcaption>Feathered ROI mask (Gaussian blend)</figcaption>
            </figure>
          </div>
          
          <!-- Frequency analysis of filtering -->
          <div class="pair">
            <figure>
              <img src="./result/2.2/fft_lowpass_sigmaL8.0.png" alt="FFT low-pass Derek sigmaL=8">
              <figcaption>FFT — low-pass Derek (σ≈8)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/fft_highpass_sigmaH4.0.png" alt="FFT high-pass Nutmeg sigmaH=4">
              <figcaption>FFT — high-pass Nutmeg (σ≈4)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/fft_hybrid.png" alt="FFT hybrid">
              <figcaption>FFT — hybrid (low + high)</figcaption>
            </figure>
          </div>
          
          <!-- Final hybrids: baseline + color strategies (280A) -->

        <div class="pair">
            <figure>
              <img src="./data/DerekPicture.jpg" alt="derek">
              <figcaption>Derek, Original</figcaption>
            </figure>
            <figure>
              <img src="./data/nutmeg.jpg" alt="nutmeg">
              <figcaption>Nutmeg, Original</figcaption>
            </figure>

          </div>
        
          <div class="pair">
            <figure>
              <img src="./result/2.2/hybrid_man_cat.png" alt="hybrid baseline">
              <figcaption>Hybrid (baseline: color-low + color-high)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/hybrid_colorLow_grayHigh_man_cat.png" alt="hybrid color low gray high">
              <figcaption>280A — color low + gray high</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/hybrid_grayLow_colorHigh_man_cat.png" alt="hybrid gray low color high">
              <figcaption>280A — gray low + color high</figcaption>
            </figure>
          </div>

          <div class="answer">
        <p>
          I first aligned Derek (low-frequency source) and Nutmeg (high-frequency source) and applied a feathered ROI mask to confine the high-pass to the face region and avoid seams. 
          A Gaussian low-pass with σ≈8 on Derek preserves coarse shape and tone while removing fine details; A Gaussian high-pass with σ≈4 on Nutmeg isolates whiskers and fur without bringing too much low-frequency content. 
          The FFT plots confirm the design: low-pass energy is concentrated near the center, high-pass energy is pushed to outer bands, and the hybrid spectrum contains both components. 
          I chose σL≈8 because smaller σL left too much mid–high frequency in Derek (competing with the cat details), while larger σL over-smoothed and produced ghosting. 
          I chose σH≈4 because larger σH leaked low frequencies from Nutmeg (double exposure), while smaller σH weakened the near-view detail. 
          The final hybrid reads as a cat face at close viewing distances and transitions to Derek from afar, matching the hybrid-image principle. 
        </p>
        <p>
          I evaluated three color strategies required for 280A: color-low + color-high, color-low + gray-high, and gray-low + color-high. 
          The best separation is color-low + gray-high because human vision is less sensitive to chroma at high spatial frequencies and grayscale high-pass avoids color fringes around whiskers. 
          Keeping color in the low-pass also preserves natural skin tones for the far-view interpretation, so I adopt color-low + gray-high as the preferred presentation for this pair. 
        </p>
      </div>
         

          
          <h4>Other hybrid (Selfie + Cat)</h4>
          <div class="pair">
            <figure>
              <img src="./data/cat.jpeg" alt="CatVisa">
              <figcaption>Cat Named Visa, Original</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/roi_mask_soft2.png" alt="soft mask self+cat">
              <figcaption>Feathered ROI mask (selfie + cat)</figcaption>
            </figure>
            <figure>
              <img src="./result/2.2/hybrid_me_cat.png" alt="hybrid me cat">
              <figcaption>Hybrid: selfie (low) + cat (high)</figcaption>
            </figure>
          </div>
        
        <div class="answer">
         <p>
          The second example uses my selfie as the low-frequency base and a cat image as the high-frequency component. 
          Alignment and Gaussian feathering masks were applied before blending to achieve smoother fusion. 

          For the second example the same pipeline with σL≈6 and σH≈4 and a feathered mask produces the intended effect: 
          the cat dominates up close, while the human outline and color read correctly at a distance. 
        </p>
      </div>


      </section>

      <!-- 2.3 Stacks -->
      <section id="p23">
        <h3>2.3 Gaussian & Laplacian Stacks</h3>
        <div class="prompt">
        <p>
          This section builds Gaussian and Laplacian stacks as preparation for multi-resolution blending in 2.4.  
          Unlike pyramids, stacks do not downsample: each Gaussian level is obtained by blurring the previous level with a Gaussian (N=5, σ=2.0), so all levels keep the original resolution.  
          The Laplacian stack is formed as L[k] = G[k] − G[k+1] with the last level set to G[−1], which isolates band-pass details at fine levels and coarse structure at deep levels.  
          I visualize representative levels (0, 2, 4) for both apple and orange to show how fine texture fades while large-scale shape remains; these panels are produced directly by the stack code without any downsampling functions.  
        </p>
      </div>

        <div class="pair">
          <figure><img src="./result/2.3/laplacian_strip_apple_0_2_4.png" alt="apple L024"><figcaption>Apple Laplacian (levels 0/2/4)</figcaption></figure>
          <figure><img src="./result/2.3/laplacian_strip_orange_0_2_4.png" alt="orange L024"><figcaption>Orange Laplacian (levels 0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/blend_strip_L024_bandmask.png" alt="blend L024"><figcaption>Blended Laplacian (0/2/4) From part 2.4 </figcaption></figure>
        </div>
        <div class="answer"><p>          
        Note that in 2.3 the true outputs are the left two Laplacian strips for apple and orange; the blended column shown on the right is generated in 2.4 using these stacks. It is included here only to aid comparison and motivate the next step.  
</p></div>
      </section>

      <!-- 2.4 Blending (+280A) -->
      <section id="p24">
        <h3>2.4 Multiresolution Blending (280A color enhancement included)</h3>

        <div class="prompt">
          <p>
            We reuse the Gaussian and Laplacian stacks from 2.3 and blend per level using a band mask stack (a softened vertical step function blurred into multiple scales).  
            The mask stack is built by taking a base band mask and forming its own Gaussian stack <code>G_mask[k]</code>, then blending each Laplacian level as <code>Lk = Mk · L_apple[k] + (1−Mk) · L_orange[k]</code>.  
            This multi-scale masking removes seams because coarse levels transition broadly while fine levels transition narrowly, matching the frequency content at each scale.  
          </p>
          <p>
            For RGB blending we use two variants: direct RGB level-wise blending, and the 280A version that blends only luminance.  
            In the 280A version, we convert to Lab, blend the L channel per level with the same band mask stack, and blend chroma (a,b) once using the smoothest level of the mask to stabilize color at coarse scales.  
            This yields clean luminance transitions and avoids color fringing or tint jumps near the seam, while preserving the natural hue of each fruit.  
            Hyperparameters: <code>N=5</code> levels, mask band ≈ 20% of width, and <code>σ_stack</code> chosen larger for the mask stack to ensure a gentle multi-scale ramp.  
          </p>
        </div>

        <h4>Oraple (vertical band mask)</h4>
        <div class="pair">
          <figure><img src="./data/orange.jpeg" alt="orange"><figcaption>Orange, Original</figcaption></figure>
          <figure><img src="./data/apple.jpeg" alt="apple"><figcaption>Apple, Original</figcaption></figure>
        </div>
        <div class="pair">
          <figure>
            <img src="./result/2.4/oraple_rgb_bandmask.png" alt="oraple rgb">
            <figcaption>Oraple (baseline) — RGB blend, per-level RGB with the same mask</figcaption>
          </figure>
          <figure>
            <img src="./result/2.4/oraple_labL_bandmask.png" alt="oraple labL">
            <figcaption>Oraple (color improved) — Lab L-only blend (per-level L; chroma blended once)</figcaption>
          </figure>
          
        </div>

        <div class="answer">
        <p>
          The vertical band mask with a Gaussian mask stack produces a smooth seam and transfers texture across scales without visible edges.
         </p>
         <p>
          The two results look similar because the apple and orange have close warm hues and the transition band is wide.  
          The Lab L-only method is preferable in theory because it blends brightness at all levels while blending color once at the coarsest scale.  
          This reduces color bleeding and keeps hue more stable near the seam, especially when the two sides differ in chroma.  
          In this pair the improvement is subtle, but zooming near the seam shows fewer color fringes for the L-only version.  
          We therefore treat the L-only blend as the more robust choice for general use.  
        </p>
      </div>

        <h4>Custom Blend #1 — Kelp (straight line mask, center split)</h4>

        <div class="pair">
          <figure><img src="./data/kelp1.jpeg" alt="k1"><figcaption>Kelp without flash, Original</figcaption></figure>
          <figure><img src="./data/kelp2.jpeg" alt="k2"><figcaption>Kelp with flash, Original</figcaption></figure>       
        </div>
        
        <div class="pair">
          <figure><img src="./result/2.4/kelp_lap_strip_A_0_2_4.png" alt="kelp A"><figcaption>Kelp A — L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/kelp_lap_strip_B_0_2_4.png" alt="kelp B"><figcaption>Kelp B — L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/kelp_lap_strip_BLEND_0_2_4.png" alt="kelp blend L"><figcaption>Blend — L(0/2/4)</figcaption></figure>
        </div>
        <div class="pair">
          <figure class="wide"><img src="./result/2.4/kelp_blend_bandmask.png" alt="kelp final" width="600"><figcaption>Final blend (kelp)</figcaption></figure>
        </div>

        <div class="answer">
          <p>
            This pair comes from the same aquarium scene taken with flash and without flash, so geometry is similar but illumination differs.  
            Images were center-cropped to match size, and we built Gaussian/Laplacian stacks with N=5 and σ_stack≈10.  
            A centered vertical band mask with ≈35% image width was turned into a Gaussian mask stack to provide scale-appropriate transitions.  
            Blending was done per level in RGB as Mk·L_A[k] + (1−Mk)·L_B[k], and the blended levels were summed to reconstruct the final image.  
            The result shows a smooth seam: the flash-lit kelp and fish on the left fade into the softer ambient look on the right without a visible boundary.  
            Fine fronds remain continuous across the seam and color halos are minimal; the wider band helps absorb exposure differences.  
          </p>
      </div>



        <h4>Custom Blend #2 — Jellyfish (ellipse, interactive)</h4>
        <!-- <div class="pair">
          <figure><img src="./result/2.4/jelly_mask_strip_0_2_4.png" alt="mask stack"><figcaption>Mask Gaussian stack (0/2/4)</figcaption></figure>
        </div> -->

  
        <div class="pair">
          <figure><img src="./data/jellyfish1.jpeg" alt="j1" width="600"><figcaption>Jellyfish as Primary Background , Original</figcaption></figure>
          <figure><img src="./data/jellyfish2.jpeg" alt="j2" width="200"><figcaption>Jellufish as Inserted, Original</figcaption></figure>       
        </div>
        <div class="pair">
          <figure><img src="./result/2.4/jelly_lap_strip_A_0_2_4.png" alt="jelly A"><figcaption>Background L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/jelly_lap_strip_B_0_2_4.png" alt="jelly B"><figcaption>Foreground L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/jelly_lap_strip_BLEND_0_2_4.png" alt="jelly blend L"><figcaption>Blend L(0/2/4)</figcaption></figure>
        </div>
        <div class="pair">
          <figure class="wide"><img src="./result/2.4/jelly_blend_ellipse_band.png" alt="jelly final band" width="600"><figcaption>Final blend (ellipse band)</figcaption></figure>
        </div>

        <div class="answer"><p>[Discussion & 280A exploration notes placeholder]</p></div>


        <div class="answer">
          <p>
            This pair comes from the same aquarium scene of two different kinds of jellyfish.  

            We create an irregular, ellipse-shaped band mask by clicking the center and two radius points, then generate a cosine-ramped band outside the ellipse to form a smooth transition region.  
            The foreground jellyfish is scaled and pasted onto a blank canvas aligned to the background, and we build Gaussian/Laplacian stacks for the background, the foreground canvas, and the mask.  
            Level-wise blending uses the mask stack exactly as in the oraple, but with <code>σ_stack</code> set larger to soften transitions around the curved boundary.  
            To avoid low-frequency color halos from the pasted foreground, we set the deepest Laplacian level of the foreground to the background’s residual before reconstruction, which keeps global illumination consistent.  
          </p>
          <p>
            The elliptical band mask produces a smooth insertion of the foreground jellyfish into the darker background without visible seams.  
            Fine tendrils remain continuous across scales, while low-frequency glow blends naturally due to the wide cosine ramp in the mask stack.  
            Overall, the composite looks coherent in both structure and illumination.  
          </p>
        </div>
      </section>
    </section>

    <hr />

    <!-- Closing -->
    <section id="takeaway">
      <h2>Most Important Thing I Learned</h2>
      <div class="answer">
        <p>
          The most important thing I learned is to see images as a mix of scales and frequencies, not just pixels.  
          Small choices—how I pad, how much I blur, where I place a mask—change the result more than I expected.  
          Writing everything from scratch made me rely on visual checks and simple comparisons, which was a good habit.  
          Multi-resolution blending felt powerful and intuitive once I watched it work on my own photos. Fantastic!
        </p>
      </div>
    </section>

  </main>

  <footer>
    <p class="muted">© <span id="y"></span> · CS280A · <a href="../index.html">Home</a></p>
  </footer>

  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>

