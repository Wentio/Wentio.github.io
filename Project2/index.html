<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS280A · Project 2 — Fun with Filters & Frequencies</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --max: 980px; }
    * { box-sizing: border-box; }
    body { margin: 24px; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif; line-height: 1.6; background:#fafafa;}
    main, header, footer { max-width: var(--max); margin: 0 auto; }
    h1, h2, h3 { line-height: 1.25; }
    .muted { color:#666; font-size: 14px; }
    .pair { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 14px; margin:14px 0;}
    figure { margin: 0; border: 1px solid #eaeaea; border-radius: 10px; padding: 10px; background: #fff; }
    figcaption { margin-top: 6px; font-size: 14px; color: #555; }
    img { max-width: 100%; height: auto; display: block; background: #f5f5f5; border-radius: 6px; }
    .answer { border-left: 4px solid #2f6feb; background: #f0f6ff; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    .prompt { border-left: 4px solid #999; background: #f7f7f7; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    nav a { text-decoration: none; color:#2f6feb; }
    hr { border: none; height: 1px; background: #eee; margin: 28px 0; }
    .toc a { color:#2f6feb; text-decoration:none; }
    .row { display:flex; gap:14px; flex-wrap:wrap; }
    .row figure { flex:1 1 300px; }
    .tight figure { padding:6px; }
    .wide figure img { width:100%; }
  </style>
</head>
<body>
  <header>
    <p class="muted"><a href="../index.html">← Back to Home</a></p>
    <h1>Project 2 — Fun with Filters & Frequencies</h1>
    <p class="muted">CS280A · Wentio Li · Sept 2025</p>
  </header>

  <main>
    <!-- TOC -->
    <section class="toc">
      <p>
        <a href="#overview"> Overview</a> ·
        <a href="#p1">Part 1: Fun with Filters</a> ·
        <a href="#p2">Part 2: Applications</a> ·
      </p>
    </section>

    <hr />

    <!-- ===================== PART 1 ===================== -->

    <!-- 1. Project Overview -->
    <section id="overview">
      <h2>Project Overview</h2>
      <div class="prompt">
        <p>
          This project explores fundamental concepts in image filtering and frequency-based techniques, serving as an introduction to computational photography and computer vision. In Part 1, we build intuition about 2D convolutions and edge detection, starting from finite difference operators, progressing to Gaussian smoothing, and finally deriving the derivative of Gaussian (DoG) filters. We also visualize gradient orientations using color mappings to better understand image structure.
        </p>
        <p>
          In Part 2, we shift focus to frequency manipulations and creative applications. We implement image sharpening with unsharp masking, construct hybrid images that reveal different content at varying viewing distances, and develop Gaussian and Laplacian stacks as the foundation for multi-resolution blending. The project culminates in seamlessly blending images—such as the classic "Oraple"—and extending the idea with irregular masks and color enhancements.  </p>     
      </div>

    <hr />

    
    <section id="p1">
      <h2>Part 1: Fun with Filters</h2>

      <!-- 1.1 -->
      <section id="p11">
        <h3>1.1 Convolutions from Scratch</h3>
        <div class="prompt">
          <p>
            I implemented 2D convolution from scratch with NumPy, first using a naive
            four-loop version and then a more efficient two-loop version that computes
            patch-wise inner products. Both implementations flip the kernel and apply zero padding so the output image matches the input
            size. 
          </p>
          <p>    
            For verification, I compared against
            <code>scipy.signal.convolve2d</code> with the same padding, and confirmed
            that the results match up to numerical precision while the two-loop version
            runs faster than the four-loop baseline. The experiments include applying a
            9×9 box filter to a grayscale selfie, and convolving with finite-difference
            kernels (<code>Dx=[1,0,-1]</code>, <code>Dy=Dx<sup>T</sup></code>) to
            visualize horizontal and vertical changes.
          </p>
 
          <p><strong>Key code snippet:</strong> </p>
        
          <pre><code class="language-python">
        def zero_pad(img, kH, kW):
            py, px = kH // 2, kW // 2
            return np.pad(img, ((py, py), (px, px)), mode="constant", constant_values=0.0)
        
        def conv2d_2loops(img, kernel):
            H, W = img.shape
            kH, kW = kernel.shape
            k = np.flipud(np.fliplr(kernel))      # flip for convolution
            padded = zero_pad(img, kH, kW)
            out = np.zeros((H, W), dtype=np.float32)
            for i in range(H):
                for j in range(W):
                    patch = padded[i:i+kH, j:j+kW]
                    out[i, j] = np.sum(patch * k) # inner product over the patch
            return out
        
        def box_filter(k=9):
            assert k % 2 == 1
            return np.ones((k, k), dtype=np.float32) / (k * k)
        
        Dx = np.array([[1, 0, -1]], dtype=np.float32); Dy = Dx.T</code></pre>
        
          <p><strong>Inputs &amp; operations.</strong> The selfie is read as grayscale and scaled to [0,1]. I convolve it with a 9×9 box filter, and also with <code>Dx</code>/<code>Dy</code> to visualize directional changes. For comparison I run SciPy with identical padding and show the MAE to demonstrate equivalence.</p>
        </div>

        <figure>
            <img src="./data/self_1.jpeg" alt="selfie" width="400" />
            <figcaption>
              Selfie at Monterey Bay Aquarium, Original
            </figcaption>
        </figure>
        <div class="pair">
          <figure><img src="./result/1.1/self_1_box9_4loops.png" alt="box 4 loops"><figcaption>4 loops, time=97.44s</figcaption></figure>
          <figure><img src="./result/1.1/self_1_box9_2loops.png" alt="box 2 loops"><figcaption>2 loops, time=13.43s</figcaption></figure>
          <figure><img src="./result/1.1/self_1_box9_scipy.png" alt="scipy ref"><figcaption>scipy.signal.convolve2d, time=0.38s</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/1.1/self_1_Dx.png" alt="Dx"><figcaption>Finite diff Dx</figcaption></figure>
          <figure><img src="./result/1.1/self_1_Dy.png" alt="Dy"><figcaption>Finite diff Dy</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            MAE (4-loops vs 2-loops): <code>8.889e-08</code>. MAE (SciPy vs 2-loops): <code>8.998e-08</code>.
          </p>
          <p>
            The three implementations produce almost identical results, confirming correct convolution (kernel flip, zero padding, same-size output). Runtimes: 4-loops = <code>97.44s</code>, 2-loops = <code>13.43s</code>, SciPy = <code>0.383s</code>. The 2-loop version is much faster than the 4-loop baseline, and SciPy is faster than both due to optimized backends.
          </p>
          <p>
            Boundaries use zero padding (equivalent to <code>boundary="fill", fillvalue=0</code> in SciPy), so the output size matches the input. Zero padding can darken edges for box filtering and create one-sided responses for <code>Dx/Dy</code>. This behavior is expected and kept here to match the specification.
          </p>
        </div>

      </section>

      <!-- 1.2 -->
      <section id="p12">
        <h3>1.2 Finite Difference Operator</h3>
        <div class="prompt">
          <p>
            For the cameraman image, I applied finite difference operators <code>Dx=[1,0,-1]</code> and <code>Dy=Dx<sup>T</sup></code> using <code>scipy.signal.convolve2d</code> with zero padding. This produces the partial derivatives in the x and y directions, which highlight horizontal and vertical intensity changes. From these two responses, I computed the gradient magnitude image as <code>sqrt(Gx^2 + Gy^2)</code> to capture edge strength independent of direction.
          </p>
          <p>
            To convert the gradient magnitude into a binary edge map, I performed a percentile-based threshold sweep (70–95%) and qualitatively selected a level that balances noise suppression with preserving important edges. Lower thresholds retain more detail but also amplify noise, while higher thresholds reduce noise at the cost of missing weaker edges. The final choice reflects this tradeoff and provides a clean yet informative edge visualization.
          </p>
        </div>
        <figure>
            <img src="./data/cameraman.png" alt="cm" width="400" />
            <figcaption>
              Cameram Man, Original
            </figcaption>
        </figure>
        <div class="pair">
          <figure><img src="./result/1.2/cameraman_Gx.png" alt="Gx"><figcaption>Gx</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_Gy.png" alt="Gy"><figcaption>Gy</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_gradmag.png" alt="gradmag"><figcaption>Gradient magnitude</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/1.2/cameraman_edge_p85.png" alt="edge p85"><figcaption>Edge (threshold ~p85)</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_edge_p90.png" alt="edge p90"><figcaption>Edge (threshold ~p90)</figcaption></figure>
          <figure><img src="./result/1.2/cameraman_edge_p95.png" alt="edge p95"><figcaption>Edge (threshold ~p95)</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            The partial derivatives clearly highlight horizontal and vertical changes, and the gradient magnitude combines them into a strong overall edge response. To create a binary edge map, I tested percentile thresholds from 85% to 95%. At 85% the edges are dense but the grass region shows many noisy dots. At 95% most background noise is suppressed, but some weaker object contours begin to disappear. The 90% threshold offers the best balance: it removes most of the grass noise while keeping the main outlines of the cameraman, tripod, and background structures intact. Therefore, the edge map at 90% is selected as the final result.
          </p>
        </div>
      </section>

      <!-- 1.3 -->
      <section id="p13">
        <h3>1.3 Derivative of Gaussian (DoG) </h3>
        <div class="prompt">
          <p>
            <strong>Method 1, Gaussian Smoothing: </strong>To reduce the noise seen with plain finite differences, I first smoothed the cameraman image with a Gaussian filter created from <code>cv2.getGaussianKernel</code> (size 7, sigma 1.5). After blurring, the gradients in x and y were computed with <code>Dx</code> and <code>Dy</code>, and the resulting magnitude image shows clearer object boundaries with much less background noise compared to Part&nbsp;1.2.
          </p> 
          <p>
            <strong>Method 2, DoG Filters:</strong> For comparison, I also built derivative-of-Gaussian (DoG) filters by convolving the Gaussian kernel directly with <code>Dx</code> and <code>Dy</code>. Applying these DoG filters to the original image produces results that closely match the “blur then differentiate” approach, confirming their equivalence while highlighting the advantage of a single convolution step.
          </p>
        </div>
        <div class="pair">
          <figure>
            <img src="./result/1.3/cameraman_blurred.png" alt="Gaussian blurred">
            <figcaption>Gaussian-blurred image (ksize=7, σ=1.5)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_Gx.png" alt="Gaussian blur then Dx">
            <figcaption>Gaussian-blur → Dx (Gx)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_Gy.png" alt="Gaussian blur then Dy">
            <figcaption>Gaussian-blur → Dy (Gy)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_blur_gradmag.png" alt="Gaussian blur grad mag">
            <figcaption>Gradient magnitude after Gaussian-blur → (Dx,Dy)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/1.3/cameraman_DoGx.png" alt="DoGx response">
            <figcaption>Convolve(I, DoGx) (x-response)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_DoGy.png" alt="DoGy response">
            <figcaption>Convolve(I, DoGy) (y-response)</figcaption>
          </figure>
          <figure>
            <img src="./result/1.3/cameraman_DoG_gradmag.png" alt="DoG grad mag">
            <figcaption>Gradient magnitude using DoG filters</figcaption>
          </figure>
        </div>

        <div class="answer">
          <p>
            The Gaussian-blur → derivative results confirm that smoothing reduces the noise seen in Part&nbsp;1.2. The gradient magnitude shows cleaner object boundaries, especially around the cameraman and tripod, while the grass region is much less noisy compared to the plain finite difference method.
          </p>
          <p>
            The DoG responses demonstrate the same effect in a more efficient way. By convolving the Gaussian kernel with <code>Dx</code> and <code>Dy</code> in advance, the DoG filters combine smoothing and differentiation into a single convolution. The resulting x- and y-responses, as well as the gradient magnitude, are visually almost identical to the blur→derivative pipeline. This verifies the mathematical equivalence of the two approaches and highlights the benefit of using DoG filters for edge detection with reduced noise.
          </p>
        </div>

      </section>
    </section>

     <section id="p14">
        <h3>1.4 Bells &amp; Whistles: HSV color space</h3>
       <div class="prompt">
        <p>
          In this extension, I computed gradient orientations using the DoG-based derivatives of the cameraman image. The orientation is given by <code>θ = arctan2(Gy, Gx)</code>, which I normalized to the range [0, π) because edge direction is π-periodic (a gradient pointing left is equivalent to one pointing right). 
          To visualize orientations, I mapped this normalized angle to hue in HSV color space, while using gradient magnitude for both saturation and value. 
          A percentile threshold on the magnitude suppresses weak gradients so that only meaningful edges are colored. 
          The resulting HSV image clearly encodes edge direction through hue variation, while brightness corresponds to edge strength.
        </p>
      </div>
        <div class="pair">
          <figure><img src="./result/1.4/cameraman_orientation_hsv.png" alt="orientation HSV"><figcaption>gradient orientation (HSV)</figcaption></figure>
        </div>
       <div class="answer">
        <p>
          The HSV visualization successfully encodes orientation as color, with different hues representing different edge directions. 
          Strong contours around the cameraman, tripod, and background structures show clear color transitions, while flat regions remain dark due to low gradient magnitude. 
          This confirms that combining orientation with magnitude in HSV space provides an intuitive way to observe both edge strength and direction in a single image.
        </p>
      </div>

      </section>
    </section>

    <hr />

    <!-- ===================== PART 2 ===================== -->
    <section id="p2">
      <h2>Part 2: Applications</h2>

      <!-- 2.1 Sharpening -->
      <section id="p21">
        <h3>2.1 Image “Sharpening” (Unsharp Mask)</h3>
        <div class="prompt">
          <p>
            In this part, I implemented the unsharp mask filter to enhance image sharpness by boosting high-frequency components. 
            A Gaussian filter is first applied as a low-pass operator to capture the smooth background (low frequencies). 
            Subtracting this blurred version from the original gives the high-pass component, which highlights edges and fine details. 
            The sharpened result is then obtained as <code>Isharp = I + α · (I − L)</code>, where <code>α</code> controls how strongly the high frequencies are added back. 
          </p>
          <p>
            For evaluation, I applied this method to the Taj Mahal image and produced the blurred, high-pass, and sharpened versions. 
            Multiple values of <code>α</code> (0.5, 1.0, 1.5) are shown to demonstrate the effect of varying the sharpening strength. 
            As a second case, I used my selfie image: the original was blurred with a Gaussian filter and then sharpened again. 
            This setup allows a direct comparison between the original and reconstructed sharpness, and PSNR values are reported to quantify the recovery quality. 
          </p>
        </div>

        <div class="pair">
          <figure>
            <img src="./result/2.1/taj_original.png" alt="taj original">
            <figcaption>Taj — original</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_lowpass.png" alt="taj low-pass">
            <figcaption>Taj — low-pass (Gaussian blur)</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_highpass.png" alt="taj high-pass">
            <figcaption>Taj — high-pass (original − low-pass)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/taj_sharp_a0.5.png" alt="taj sharp a0.5">
            <figcaption>Taj — sharpen α=0.5</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_sharp_a1.0.png" alt="taj sharp a1.0">
            <figcaption>Taj — sharpen α=1.0</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/taj_sharp_a1.5.png" alt="taj sharp a1.5">
            <figcaption>Taj — sharpen α=1.5</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/self_original.png" alt="self original">
            <figcaption>Selfie — original</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sigma1.8.png" alt="self blur">
            <figcaption>Selfie — blurred (σ=1.8)</figcaption>
          </figure>
        </div>
        
        <div class="pair">
          <figure>
            <img src="./result/2.1/self_blur_sharp_a0.5_s1.8.png" alt="self sharp a0.5">
            <figcaption>Selfie — sharpen α=0.5</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sharp_a1.0_s1.8.png" alt="self sharp a1.0">
            <figcaption>Selfie — sharpen α=1.0</figcaption>
          </figure>
          <figure>
            <img src="./result/2.1/self_blur_sharp_a1.5_s1.8.png" alt="self sharp a1.5">
            <figcaption>Selfie — sharpen α=1.5</figcaption>
          </figure>
        </div>
        
        <div class="metrics">
          <pre>
        PSNR (reference = self_original.png)
        blur (σ=1.8): PSNR = 36.182 dB
        sharpen (α=0.5, σ=1.8): PSNR = 37.522 dB
        sharpen (α=1.0, σ=1.8): PSNR = 38.742 dB
        sharpen (α=1.5, σ=1.8): PSNR = 39.576 dB
          </pre>
        </div>

        <div class="answer"><p>[Observations placeholder]</p></div>
      </section>

      <!-- 2.2 Hybrids (+280A color experiments) -->
      <section id="p22">
        <h3>2.2 Hybrid Images (+ 280A color experiments)</h3>
        <div class="prompt"><p>[Write-up placeholder — alignment, σ choices, color choices]</p></div>

        <h4>Full pipeline (Derek + Nutmeg)</h4>
        <div class="pair">
          <figure><img src="./result/2.2/fft_input_low_Derek.png" alt="fft low Derek"><figcaption>FFT — low (Derek)</figcaption></figure>
          <figure><img src="./result/2.2/fft_input_high_Nutmeg.png" alt="fft high cat"><figcaption>FFT — high (Nutmeg)</figcaption></figure>
          <figure><img src="./result/2.2/fft_lowpass_sigmaL8.0.png" alt="fft lowpass"><figcaption>Low-pass (σ≈8)</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/2.2/fft_highpass_sigmaH4.0.png" alt="fft highpass"><figcaption>High-pass (σ≈4)</figcaption></figure>
          <figure><img src="./result/2.2/fft_hybrid.png" alt="fft hybrid"><figcaption>FFT — hybrid</figcaption></figure>
          <figure><img src="./result/2.2/fft_hybrid.png" alt="dup" style="opacity:0"></figure>
        </div>
        <div class="pair">
          <figure><img src="./result/2.2/hybrid_man_cat.png" alt="hybrid base"><figcaption>Hybrid (baseline)</figcaption></figure>
          <figure><img src="./result/2.2/hybrid_colorLow_grayHigh_man_cat.png" alt="cL gH"><figcaption>280A: color-low + gray-high</figcaption></figure>
          <figure><img src="./result/2.2/hybrid_colorLow_colorHigh_man_cat.png" alt="cL cH"><figcaption>280A: color-low + color-high</figcaption></figure>
        </div>

        <h4>Other hybrids</h4>
        <div class="pair">
          <figure><img src="./result/2.2/hybrid_me_cat.png" alt="me+cat"><figcaption>Hybrid: me + cat</figcaption></figure>
        </div>

        <div class="answer"><p>[Notes on perception & distance placeholder]</p></div>
      </section>

      <!-- 2.3 Stacks -->
      <section id="p23">
        <h3>2.3 Gaussian & Laplacian Stacks</h3>
        <div class="prompt"><p>[Write-up placeholder — stacks vs pyramids, N=5, σ settings]</p></div>
        <div class="pair">
          <figure><img src="./result/2.3/laplacian_strip_apple_0_2_4.png" alt="apple L024"><figcaption>Apple Laplacian (levels 0/2/4)</figcaption></figure>
          <figure><img src="./result/2.3/laplacian_strip_orange_0_2_4.png" alt="orange L024"><figcaption>Orange Laplacian (levels 0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/blend_strip_L024_bandmask.png" alt="blend L024"><figcaption>Blended Laplacian (0/2/4)</figcaption></figure>
        </div>
        <div class="answer"><p>[Observations placeholder]</p></div>
      </section>

      <!-- 2.4 Blending (+280A) -->
      <section id="p24">
        <h3>2.4 Multiresolution Blending (+ 280A)</h3>
        <div class="prompt"><p>[Write-up placeholder — mask stack, band width, reconstruction]</p></div>

        <h4>Oraple (vertical band mask)</h4>
        <div class="pair">
          <figure><img src="./result/2.4/oraple_rgb_bandmask.png" alt="oraple rgb"><figcaption>Final: RGB band-mask blend</figcaption></figure>
          <figure><img src="./result/2.4/oraple_labL_bandmask.png" alt="oraple labL"><figcaption>280A: Lab(L-only) band-mask</figcaption></figure>
        </div>

        <h4>Custom Blend #1 — Kelp (band mask, center split)</h4>
        <div class="pair">
          <figure><img src="./result/2.4/kelp_lap_strip_A_0_2_4.png" alt="kelp A"><figcaption>Kelp A — L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/kelp_lap_strip_B_0_2_4.png" alt="kelp B"><figcaption>Kelp B — L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/kelp_lap_strip_BLEND_0_2_4.png" alt="kelp blend L"><figcaption>Blend — L(0/2/4)</figcaption></figure>
        </div>
        <div class="pair">
          <figure class="wide"><img src="./result/2.4/kelp_blend_bandmask.png" alt="kelp final"><figcaption>Final blend (kelp)</figcaption></figure>
        </div>

        <h4>Custom Blend #2 — Jellyfish (ellipse, interactive)</h4>
        <!-- <div class="pair">
          <figure><img src="./result/2.4/jelly_mask_strip_0_2_4.png" alt="mask stack"><figcaption>Mask Gaussian stack (0/2/4)</figcaption></figure>
        </div> -->
        <div class="pair">
          <figure><img src="./result/2.4/jelly_lap_strip_A_0_2_4.png" alt="jelly A"><figcaption>Background L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/jelly_lap_strip_B_0_2_4.png" alt="jelly B"><figcaption>Foreground L(0/2/4)</figcaption></figure>
          <figure><img src="./result/2.4/jelly_lap_strip_BLEND_0_2_4.png" alt="jelly blend L"><figcaption>Blend L(0/2/4)</figcaption></figure>
        </div>
        <div class="pair">
          <figure class="wide"><img src="./result/2.4/jelly_blend_ellipse_band.png" alt="jelly final band"><figcaption>Final blend (ellipse band)</figcaption></figure>
        </div>

        <div class="answer"><p>[Discussion & 280A exploration notes placeholder]</p></div>
      </section>
    </section>

    <hr />

    <!-- Closing -->
    <section id="takeaway">
      <h2>Most Important Thing I Learned</h2>
      <div class="prompt"><p>[Short reflection placeholder]</p></div>
    </section>

  </main>

  <footer>
    <p class="muted">© <span id="y"></span> · CS280A · <a href="../index.html">Home</a></p>
  </footer>

  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>

