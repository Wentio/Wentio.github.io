<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS180/280A · Project 3 — Stitching Photo Mosaics</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --max: 980px; }
    * { box-sizing: border-box; }
    body { margin: 24px; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif; line-height: 1.6; background:#fafafa;}
    main, header, footer { max-width: var(--max); margin: 0 auto; }
    h1, h2, h3 { line-height: 1.25; }
    .muted { color:#666; font-size: 14px; }
    .pair, .row { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 14px; margin:14px 0;}
    figure { margin: 0; border: 1px solid #eaeaea; border-radius: 10px; padding: 10px; background: #fff; }
    figcaption { margin-top: 6px; font-size: 14px; color: #555; }
    img { max-width: 100%; height: auto; display: block; background: #f5f5f5; border-radius: 6px; }
    nav a, .toc a { text-decoration: none; color:#2f6feb; }
    hr { border: none; height: 1px; background: #eee; margin: 28px 0; }
    .tight figure { padding:6px; }
    .wide figure img { width:100%; }
    .downloads a { display:inline-block; margin-right:12px; font-size:14px; }
    .grid-3 { display:grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap:14px; }
  </style>
</head>
<body>
  <header>
    <p class="muted"><a href="../index.html">← Back to Home</a></p>
    <h1>Project 3 — Stitching Photo Mosaics</h1>
    <p class="muted">CS180/280A · Wentio Li · Fall 2025</p>
  </header>

  <main>
    <!-- TOC -->
    <section class="toc">
      <p>
        <a href="#overview">Overview</a> ·
        <a href="#a1">A.1 Shoot the Pictures</a> ·
        <a href="#a2">A.2 Recover Homographies</a> ·
        <a href="#a3">A.3 Warp the Images & Rectification</a> ·
        <a href="#a4">A.4 Blend into a Mosaic</a> ·
        <a href="#a5">A.5 Bells & Whistles</a> · 
        <a href="#b1">B.1 Harris Corners</a> · 
        <a href="#b2">B.2 Descriptors</a> · 
        <a href="#b3">B.3 Matching</a> · 
        <a href="#b4">B.4 RANSAC &amp; Auto Mosaics</a> · 
        <a href="#b5">B.5 Bells &amp; Whistles</a>

        
      </p>
    </section>

    <hr />

    <!-- ============== OVERVIEW ============== -->
    <section id="overview">
      <h2>Project Overview</h2>
      <div class="prompt">
        <p>
          In this project, we explore image warping and mosaicing — a classic problem in computer vision. 
          The A part of project consists of several steps: first capturing suitable images with overlapping fields of view, then recovering homographies between pairs of images using manually selected correspondences. 
          Next, we perform inverse warping to align images under projective transformations and experiment with rectification to verify correctness. 
          Finally, we blend the warped images into mosaics using weighted averaging or feathering techniques, and optionally extend to <em>cylindrical projection</em> for wide panoramas.
        </p>
        <p>
            In part B of the project, the goal is to automatically detect and match feature points across overlapping images and build panoramas without manual correspondences. 
            The pipeline includes detecting corner features using the Harris detector, selecting reliable points with Adaptive Non-Maximal Suppression (ANMS), extracting local patch descriptors, matching descriptors between image pairs, and computing a robust homography using RANSAC. 
            After computing the transformation, I reuse the warping and blending code from part A to produce automatic mosaics. 
            Finally, I explore an additional extension that uses multiscale feature detection to improve matching quality and robustness across different scene structures.
        </p>

  
      </div>
    </section>


    <hr />

    <!-- ============== A1 ============== -->
    <section id="a1">
      <h2>A.1 Shoot the Pictures</h2>
      <div class="prompt">
        <p>
          This set of photos captures the sunset canyon scenery at the classic scenic spot in Yellowstone National Park.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/river1.jpeg" alt="river1 original" />
          <figcaption>left, canyon (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/river2.jpeg" alt="river2 original" />
          <figcaption>medium, canyon (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/river3.jpeg" alt="river3 original" />
          <figcaption>right, canyon (source)</figcaption>
        </figure>
      </div>
      <div class="prompt">
        <p>
          Another set of photos in Yellowstone National Park: the West Thumb Lake.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/lake_left.jpeg" alt="lake1 original" />
          <figcaption>left, lake (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/lake_right.jpeg" alt="lake2 original" />
          <figcaption>right, lake (source)</figcaption>
        </figure>
      </div>
      <div class="prompt">
        <p>
          Another set of photos in Yellowstone National Park: the Norris Basin.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/basin_left.jpeg" alt="basin original" />
          <figcaption>left, basin (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/basin_right.jpeg" alt="basin2 original" />
          <figcaption>right, basin (source)</figcaption>
        </figure>
      </div>
    </section>

    <hr />

    <!-- ============== A2 ============== -->
    <section id="a2">
      <h2>A.2 Recover Homographies</h2>
      <div class="prompt">
        <p>
            To align two images, we first need to estimate the projective transformation between them, represented by a 3×3 homography matrix. 
    I implemented <code>computeH(im1_pts, im2_pts)</code> based on the normalized Direct Linear Transform (DLT) algorithm. 
    The method takes pairs of manually selected correspondences from two images, normalizes their coordinates to improve numerical stability, 
    constructs an overdetermined linear system <code>Ah = 0</code>, and solves it using singular value decomposition (SVD). 
    The resulting matrix is then denormalized to produce the final homography. 
        </p>
      </div>
<pre><code class="language-python">
def computeH(im1_pts, im2_pts, use_normalization=True):
    im1_pts = np.asarray(im1_pts, dtype=np.float64)
    im2_pts = np.asarray(im2_pts, dtype=np.float64)
    assert im1_pts.shape == im2_pts.shape and im1_pts.shape[0] >= 4
    if use_normalization:
        T1, p1 = normalize_points(im1_pts)
        T2, p2 = normalize_points(im2_pts)
    else:
        T1, p1 = np.eye(3), im1_pts
        T2, p2 = np.eye(3), im2_pts
    A = build_A_normDLT(p1, p2)
    _,_,Vt = np.linalg.svd(A, full_matrices=False)
    Hn = Vt[-1].reshape(3,3)
    H = np.linalg.inv(T2) @ Hn @ T1
    if abs(H[2,2]) > 1e-12: H = H / H[2,2]
    return H
    </code></pre>
      
      <figure>
          <img src="./result/2_correspond.png" alt="corr" width="700" />
          <figcaption>Correspondence Visualization, left and medium</figcaption>
        </figure>
      <!-- A.2 · Text 2 -->
      <p>
      <pre>
      H =
        [[ 1.312538  0.091343 -604.306458]
         [ 0.199424  1.271263 -163.161362]
         [ 0.000249  0.000053   1.000000]]
        mean reprojection error: 1.942 px
      </pre>
      </p>
      
      <div class="answer">
        <p>
        The estimated homography aligns the two river images accurately with an average reprojection error under 2 pixels. 
        The transformation captures both rotation and perspective distortion, as seen from the horizontal shift and slight scaling across the canyon. 
        The correspondences (shown above) match key geometric structures, confirming that the normalization and least-squares SVD approach effectively recovered a stable mapping.
        </p>
      </div>

    </section>

    

    <hr />

    <!-- ============== A3 ============== -->
    <section id="a3">
      <h2>A.3 Warp the Images &amp; Rectification</h2>


      <!-- A.3 · Text 1 + Code -->
      <p>
      I warp images by inverse mapping with a precomputed homography. I first predict the output canvas by transforming the four source corners and build a shift transform so all warped pixels land at nonnegative coordinates. For each output pixel, I map back with H⁻¹ to sample the source. I implement two samplers: nearest neighbor (round to the closest pixel) and bilinear (weighted average of four neighbors). Both return an alpha mask indicating valid samples, which is useful for preview overlays and later blending. 
      </p>

<pre><code class="language-python">
def warpImageNearestNeighbor(im, H, out_shape=None, out_T=None):
    if out_shape is None or out_T is None:
        out_shape, T = make_canvas_and_transform(im.shape, H)
    else:
        T = out_T
    Htot = T @ H
    Hinv = np.linalg.inv(Htot)
    oh, ow = out_shape
    yy, xx = np.meshgrid(np.arange(oh), np.arange(ow), indexing="ij")
    tgt = np.stack([xx, yy, np.ones_like(xx)], axis=-1).reshape(-1,3)
    src = (Hinv @ tgt.T).T
    src_xy = src[:,:2] / src[:,2:3]
    su = np.rint(src_xy[:,0]).astype(int)
    sv = np.rint(src_xy[:,1]).astype(int)
    h, w = im.shape[:2]
    inside = (su&gt;=0)&amp;(su&lt;w)&amp;(sv&gt;=0)&amp;(sv&lt;h)
    out = np.zeros((oh, ow, im.shape[2] if im.ndim==3 else 1), dtype=np.float32)
    if im.ndim==3:
        out = out.reshape(-1,3)
        out[inside] = im[sv[inside], su[inside]]
        out = out.reshape(oh, ow, 3)
    else:
        out = out.reshape(-1,1)
        out[inside,0] = im[sv[inside], su[inside]]
        out = out.reshape(oh, ow)
    alpha = inside.astype(np.float32).reshape(oh, ow)
    return out, alpha, T

def warpImageBilinear(im, H, out_shape=None, out_T=None):
    if out_shape is None or out_T is None:
        out_shape, T = make_canvas_and_transform(im.shape, H)
    else:
        T = out_T
    Htot = T @ H
    Hinv = np.linalg.inv(Htot)
    oh, ow = out_shape
    yy, xx = np.meshgrid(np.arange(oh), np.arange(ow), indexing="ij")
    tgt = np.stack([xx, yy, np.ones_like(xx)], axis=-1).reshape(-1,3)
    src = (Hinv @ tgt.T).T
    uv = src[:,:2] / src[:,2:3]
    u = uv[:,0]; v = uv[:,1]
    x0 = np.floor(u).astype(int); x1 = x0 + 1
    y0 = np.floor(v).astype(int); y1 = y0 + 1
    ax = u - x0; ay = v - y0
    h, w = im.shape[:2]
    valid = (x0&gt;=0)&amp;(x1&lt;w)&amp;(y0&gt;=0)&amp;(y1&lt;h)
    out = np.zeros((oh, ow, im.shape[2] if im.ndim==3 else 1), dtype=np.float32).reshape(-1, (3 if im.ndim==3 else 1))
    if im.ndim==3:
        I00 = np.zeros((tgt.shape[0],3), np.float32); I10 = I00.copy(); I01 = I00.copy(); I11 = I00.copy()
        idx = valid.nonzero()[0]
        if idx.size:
            I00[idx] = im[y0[idx], x0[idx]]
            I10[idx] = im[y0[idx], x1[idx]]
            I01[idx] = im[y1[idx], x0[idx]]
            I11[idx] = im[y1[idx], x1[idx]]
        w00 = (1-ax)*(1-ay); w10 = ax*(1-ay); w01 = (1-ax)*ay; w11 = ax*ay
        out = (I00*w00[:,None] + I10*w10[:,None] + I01*w01[:,None] + I11*w11[:,None])
        out = out.reshape(oh, ow, 3)
    else:
        I00 = np.zeros((tgt.shape[0],1), np.float32); I10 = I00.copy(); I01 = I00.copy(); I11 = I00.copy()
        idx = valid.nonzero()[0]
        if idx.size:
            I00[idx,0] = im[y0[idx], x0[idx]]
            I10[idx,0] = im[y0[idx], x1[idx]]
            I01[idx,0] = im[y1[idx], x0[idx]]
            I11[idx,0] = im[y1[idx], x1[idx]]
        w00 = (1-ax)*(1-ay); w10 = ax*(1-ay); w01 = (1-ax)*ay; w11 = ax*ay
        out = (I00*w00[:,None] + I10*w10[:,None] + I01*w01[:,None] + I11*w11[:,None]).reshape(oh, ow)
    alpha = valid.astype(np.float32).reshape(oh, ow)
    return out, alpha, T
</code></pre>


   <h3>Nearest Neighbor Warping</h3>
      <div class="row" style="align-items: start;">
        <figure style="flex:1;">
          <img src="./result/im1_warp_to_im2_nn.png" alt="im1 warped to im2 using nearest neighbor" style="height:420px; width:auto;" />
          <figcaption>Warped image, nearest neighbor interpolation</figcaption>
        </figure>
        <figure style="flex:1;">
          <img src="./result/3_overlay_nn.png" alt="overlay of nearest neighbor warped image on im2" style="height:420px; width:auto;" />
          <figcaption>Overlay of warped left on medium, NN</figcaption>
        </figure>
      </div>
      
      <h3>Bilinear Warping</h3>
      <div class="row" style="align-items: start;">
        <figure style="flex:1;">
          <img src="./result/im1_warp_to_im2_bilinear.png" alt="im1 warped to im2 using bilinear interpolation" style="height:420px; width:auto;" />
          <figcaption>Warped image, bilinear interpolation</figcaption>
        </figure>
        <figure style="flex:1;">
          <img src="./result/3_overlay_bl.png" alt="overlay of bilinear warped image on im2" style="height:420px; width:auto;" />
          <figcaption>Overlay of warped left on medium, bilinear</figcaption>
        </figure>
      </div>
      
      <div class="answer">
        <p>
          Both warping methods correctly project the first image into the coordinate frame of the second image based on the recovered homography. 
          The nearest neighbor result appears sharp but slightly jagged along edges, as it assigns each target pixel the value of the closest source pixel. 
          In contrast, bilinear interpolation produces smoother transitions and fewer aliasing artifacts, particularly along the canyon ridges and sky boundaries. 
          The overlays show that alignment is consistent and geometry is preserved. 
          The bilinear method offers higher visual quality, while the nearest neighbor version is faster to compute and easier to verify numerically.
        </p>
      </div>

    </section>

    <hr />

    <!-- ============== A4 ============== -->
    <section id="a4">
      <h2>A.4 Blend the Images into a Mosaic</h2>

      <p>
    To create the mosaic, I used the left image and the medium image as the reference pair. 
    The homography computed in the previous step warps <code>left canyon</code> into the coordinate frame of <code>medium canyon</code>. 
    I first generate a shared canvas that fully contains both warped images, then warp <code>left </code> using bilinear interpolation while keeping <code>medium</code> fixed. 
    Each image is assigned a soft alpha mask that decreases toward the borders, and the final mosaic is obtained through weighted averaging of the overlapping regions to achieve smooth blending.
    </p>
      
      <div class="row">
        <figure>
          <img src="./result/mosaic_river12_im1warped.png" alt="mosaic: im1 warped into im2" />
          <figcaption>left, wrapped</figcaption>
        </figure>
        <figure>
          <img src="./result/mosaic_river12_im2oncanvas.png" alt="mosaic: im2 on canvas" />
          <figcaption>meidum, on canvas</figcaption>
        </figure>
      </div>
      <figure>
          <img src="./result/mosaic_river12.png" alt="mosaic blended result" width="700" />
          <figcaption>mosaic, left and medium</figcaption>
        </figure>

      <div class="answer">
        <p>
        The resulting mosaic shows a natural alignment between the canyon walls and the river. 
        The weighted alpha blending effectively removes hard seams and balances brightness across overlapping areas. 
        Some slight ghosting appears where minor geometric differences exist between shots, but overall the transition between <code>river1</code> and <code>river2</code> is smooth and visually coherent. 
        The final output successfully demonstrates projective registration and simple feathering-based compositing.
        </p>
      </div>
        <div class="row">
        <figure>
          <img src="./result/mosaic_lake_left_warped.png" alt="mosaic: im1 warped into im2 l" />
          <figcaption>left, wrapped</figcaption>
        </figure>
        <figure>
          <img src="./result/mosaic_lake_right_oncanvas.png" alt="mosaic: im2 on canvas l" />
          <figcaption>right, on canvas</figcaption>
        </figure>
      </div>
      <figure>
          <img src="./result/mosaic_lake_lr.png" alt="mosaic blended result l" width="800" />
          <figcaption>mosaic, left and right</figcaption>
        </figure>

      <div class="answer">
        <p>
        For the West Thumb Lake, the resulting mosaic is noticeably cleaner and more seamless than the canyon example. 
        This improvement likely comes from the simpler scene geometry — the lake and horizon provide broad, consistent features with fewer sharp depth changes. 
        Because the landscape is more distant and has smoother textures, the alignment and blending steps were easier and produced a clearer final image.
        </p>

      </div>
      
    </section>

    <hr />

    <!-- ============== A5 ============== -->
    <section id="a5">
      <h2>A.5 Bells &amp; Whistles</h2>

      <!-- A.5 · Text 1 -->
<p>
For the final step, I extended the planar mosaicing pipeline to a cylindrical projection to build a wide panorama of canyon left, medium, and right.
Each image is first warped onto a virtual cylinder using inverse mapping and bilinear sampling, where horizontal pixel coordinates are mapped to rotation angles. 
This representation converts camera rotation into horizontal translation, allowing simpler alignment and reducing stretching near the top and bottom edges. 
I then use normalized cross-correlation on the gray and alpha-masked regions to estimate the horizontal offsets between adjacent cylindrical images. 
Finally, I place all warped images on a shared canvas and blend them with soft alpha weights for a seamless transition.
</p>

<p>
A cylindrical projection is particularly suitable here because the camera mainly rotated horizontally while capturing the canyon. 
It preserves straight vertical structures and prevents extreme distortion that would appear in a planar projection. 
In contrast, spherical mapping would be necessary only if the capture involved significant up–down rotation. 
</p>

<p> 
The focal length in pixels was derived from the EXIF data of the iPhone 16 Pro main camera (24 mm lens, 4284 px width, 13.4 mm sensor width): 
<span>f<sub>px</sub> ≈ 24 × (4284 / 13.4) ≈ 7700 px</span>.
And all three photos were taken with the same focal length.
</p>

      <div class="wide">
        <figure>
          <img src="./result/cylindrical_panorama_river123.png" alt="cylindrical panorama river123" width="700" />
          <figcaption>cylindrical panorama of canyon, left/medium/right </figcaption>
        </figure>
      </div>

<div class="answer">
  <p>
  The cylindrical panorama successfully combines all three views into a continuous wide-angle image. 
  The curved projection removes the perspective stretching that would occur in a planar mosaic and produces a natural sense of depth across the canyon. 
  Vertical structures remain nearly straight, confirming that the cylindrical model fits the mostly horizontal camera motion well. 
</p>

<p> 
  However, compared with the two-image mosaic in Part A.4, the alignment appears slightly less precise near the central ridge. 
  This small mismatch likely comes from imperfect offset estimation in normalized cross-correlation, as cylindrical warping changes local geometry and slightly reduces texture consistency across the overlap. 
  Despite that, the overall transition is smooth, and the result illustrates the benefit of cylindrical projection for wide panoramas.
  </p>
</div>

    </section>

    <hr />

<!-- ============== B1 ============== -->
<section id="b1">
  <h2>B.1 Harris Corner Detection</h2>

 <div class="prompt">
  <p>
    In this step, I used the provided helper function <code>get_harris_corners</code> to compute the corner strength map and extract local maxima. 
    To avoid unstable responses near the border, all points within 20 pixels from the image edges were discarded. 
  </p>
  <p>
    I also applied Adaptive Non-Maximal Suppression (ANMS) to retain only well-distributed strong corners. 
    The implementation first sorts all corners by response strength, 
    then for each point computes the distance to the nearest stronger neighbor whose response is higher by a ratio <code>c</code> (set to 0.9). 
    The points with the largest suppression radii are chosen as the final keypoints, typically the top 500. 
    Also, before running ANMS, I kept only the strongest 2000 raw corners to reduce computation cost.
  </p>
</div>


  <h3>Harris corners (all candidates)</h3>
  <div class="row">
    <figure>
      <img src="./result/B1/river_left_harris_all.jpg" alt="left image, Harris corners (all)" />
      <figcaption>left, Harris corners (all)</figcaption>
    </figure>
    <figure>
      <img src="./result/B1/river_right_harris_all.jpg" alt="right image, Harris corners (all)" />
      <figcaption>right, Harris corners (all)</figcaption>
    </figure>
  </div>

  <h3>ANMS-selected corners</h3>
  <div class="row">
    <figure>
      <img src="./result/B1/river_left_anms_500.jpg" alt="left image, ANMS 500" />
      <figcaption>left, ANMS (top 500)</figcaption>
    </figure>
    <figure>
      <img src="./result/B1/river_right_anms_500.jpg" alt="right image, ANMS 500" />
      <figcaption>right, ANMS (top 500)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="answer">
  <p>
    The Harris detector identifies a dense set of responses across textured regions, especially along the canyon walls where gradients change sharply. 
    After applying ANMS, only the most spatially separated strong corners remain, 
    and most of them on along the canyon walls and river.
    Only very limited points are on the sky (where the bound is more vague).
    The selected 500 points cover both foreground and background areas, providing reliable features for matching in later steps.
  </p>
</div>

  </div>
</section>

<hr />

<!-- ============== B2 ============== -->
<section id="b2">
  <h2>B.2 Feature Descriptor Extraction</h2>

  <div class="prompt">
  <p>
    I extract axis-aligned descriptors by sampling a 40×40 window around each ANMS keypoint and downsampling it to 8×8. 
    The 40×40 crop is taken with sub-pixel centers and symmetric “reflect” padding to avoid border artifacts. 
    For downsampling, the 8×8 patch approximates a blurred average and improves descriptor stability.
  </p>
  <p>
    Each 8×8 patch is bias/gain normalized: subtract the mean, divide by the standard deviation with <code>eps=1e-6</code> for numerical safety. 
    I optionally clip extremes at ±3σ before normalization to reduce outlier influence from specular highlights or noise. 
    Descriptors are stored as row vectors of length 64 and a montage is generated by min-max scaling each patch and tiling 5×5 for visualization. 
    With <code>max_pts=800</code> per image the resulting matrices have shape <code>(N,64)</code>, ready for nearest-neighbor matching in B.3.
  </p>
</div>


  <div class="row">
    <figure>
      <img src="./result/B2/river_left_desc_montage.jpg" alt="left descriptors montage" />
      <figcaption>left, descriptor montage (normalized 8×8 samples)</figcaption>
    </figure>
    <figure>
      <img src="./result/B2/river_right_desc_montage.jpg" alt="right descriptors montage" />
      <figcaption>right, descriptor montage (normalized 8×8 samples)</figcaption>
    </figure>
  </div>

</section>

<hr />

<!-- ============== B3 ============== -->
<section id="b3">
  <h2>B.3 Feature Matching</h2>

  <div class="prompt">
    <p>
    I match features by Euclidean distance in descriptor space and use the Lowe ratio test to reject ambiguous pairs.
    Distances between all descriptor pairs are computed in one shot using the expanded L2 formula (‖a‖² + ‖b‖² − 2a·b), 
    For each keypoint on the left image I find its two nearest neighbors on the right image and keep the match only if d₁/d₂ < 0.70.
    This keeps points with a clear best correspondence and removes many false positives on repetitive textures.
    For visualization I draw either all accepted matches, and the top-50 smallest-distance pairs on a side-by-side canvas.
    I also experimented with a mutual-nearest variant where a pair is kept only if each is the other’s best neighbor; 
    this yields a slightly cleaner but smaller set, and the downstream RANSAC step can succeed with either option.
  </p>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B3/river_LR_matches_ratio70.jpg" alt="matches after ratio test 0.70" width="700" />
      <figcaption>matches after ratio test (τ = 0.70)</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B3/river_LR_matches_top50.jpg" alt="top-50 raw matches" width="700" />
      <figcaption>top-50 nearest matches (visualization)</figcaption>
    </figure>
  </div>

  <div class="answer">
  <p>
    With the ratio threshold at 0.7, 160 matches were retained between the two images. 
    Most correspondences align along the canyon ridges and river boundaries that provide strong gradients and distinctive patterns. 
  </p>
</div>
</section>

<hr />

<!-- ============== B4 ============== -->
<section id="b4">
  <h2>B.4 RANSAC for Robust Homography and Auto Mosaics</h2>

  <<div class="prompt">
  <p>
    In Part B.2 and B.3, I have build tentative correspondences from the 8×8 descriptors using L2 distance and Lowe’s ratio test with τ = 0.7. 
    In this step, I estimate a projective homography with 4-point RANSAC. 
    Each iteration samples 4 pairs, solves a normalized DLT, and scores with the symmetric transfer error (forward and backward) in pixels. 
    The inlier threshold is 5 px and I run 10,000 iterations to stabilize the consensus. 
    After selecting the best model I refit H on all inliers to reduce bias. 
    The refined H is then used to warp the left image into the right frame with the Part-A inverse warper and bilinear sampling. 
    A shared canvas and soft-alpha feathering produce the final automatic mosaics for the canyon (left+medium), lake (left+right), and basin (left+right).
  </p>
</div>

<div class="row">
  <figure>
    <img src="./result/B4/mosaic_river12_auto.jpg" alt="auto mosaic river (left+medium)" width="700" />
    <figcaption>
      auto mosaic: canyon (left + medium) ·
      <a href="#a4" title="Jump to A.4 manual stitch for canyon">compare with manual (A.4)</a>
    </figcaption>
  </figure>
</div>

<div class="row">
  <figure>
    <img src="./result/B4/mosaic_lake_auto.jpg" alt="auto mosaic lake" width="800" />
    <figcaption>
      auto mosaic: lake (left + right) ·
      <a href="#a4" title="Jump to A.4 manual stitch for lake">compare with manual (A.4)</a>
    </figcaption>
  </figure>
</div>

<div class="row">
  <figure>
    <img src="./result/B4/mosaic_basin_auto.jpg" alt="auto mosaic basin" width="800" />
    <figcaption>auto mosaic: basin (left + right)</figcaption>
  </figure>
</div>


  <div class="answer">
    <p>
      Across scenes, RANSAC eliminates outliers and yields homographies close to the manual version. 
      The lake and basin panoramas appear especially clean due to simpler geometry and larger viewing distance.
    </p>
  </div>
</section>

<hr />

<!-- ============== B5 ============== -->
<section id="b5">
  <h2>B.5 Bells &amp; Whistles</h2>

  <div class="prompt">
    <p>
      I add multiscale processing: corners are detected over multiple smoothing levels and descriptors are formed per scale. 
      This improves feature coverage across details of different sizes and increases match robustness.
    </p>
  </div>

  <h3>Multiscale examples and overlays</h3>
  <div class="row">
    <figure>
      <img src="./result/B5/river1_multiscale_montage.jpg" alt="river1 multiscale montage" />
      <figcaption>river1 multiscale responses / selections</figcaption>
    </figure>
    <figure>
      <img src="./result/B5/river2_multiscale_montage.jpg" alt="river2 multiscale montage" />
      <figcaption>river2 multiscale responses / selections</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B5/river1_multiscale_overlay.jpg" alt="river1 multiscale overlay" />
      <figcaption>river1 multiscale overlay on image</figcaption>
    </figure>
    <figure>
      <img src="./result/B5/river2_multiscale_overlay.jpg" alt="river2 multiscale overlay" />
      <figcaption>river2 multiscale overlay on image</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B5/river_multi.jpg" alt="multi-image panorama (bonus)" width="700" />
      <figcaption>multi-image panorama (autostitched)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      Multiscale detection supplies more distinctive features on both coarse structures and fine textures, reducing failure cases in low-texture areas and improving overall stitch quality.
    </p>
  </div>
</section>


  </main>

  <footer>
    <p class="muted">© <span id="y"></span> · CS180/280A · <a href="../index.html">Home</a></p>
  </footer>

  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>

