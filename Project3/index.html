<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS180/280A · Project 3 — Stitching Photo Mosaics</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --max: 980px; }
    * { box-sizing: border-box; }
    body { margin: 24px; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif; line-height: 1.6; background:#fafafa;}
    main, header, footer { max-width: var(--max); margin: 0 auto; }
    h1, h2, h3 { line-height: 1.25; }
    .muted { color:#666; font-size: 14px; }
    .pair, .row { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 14px; margin:14px 0;}
    figure { margin: 0; border: 1px solid #eaeaea; border-radius: 10px; padding: 10px; background: #fff; }
    figcaption { margin-top: 6px; font-size: 14px; color: #555; }
    img { max-width: 100%; height: auto; display: block; background: #f5f5f5; border-radius: 6px; }
    nav a, .toc a { text-decoration: none; color:#2f6feb; }
    hr { border: none; height: 1px; background: #eee; margin: 28px 0; }
    .tight figure { padding:6px; }
    .wide figure img { width:100%; }
    .downloads a { display:inline-block; margin-right:12px; font-size:14px; }
    .grid-3 { display:grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap:14px; }
  </style>
</head>
<body>
  <header>
    <p class="muted"><a href="../index.html">← Back to Home</a></p>
    <h1>Project 3 — Stitching Photo Mosaics</h1>
    <p class="muted">CS180/280A · Wentio Li · Fall 2025</p>
  </header>

  <main>
    <!-- TOC -->
    <section class="toc">
      <p>
        <a href="#overview">Overview</a> ·
        <a href="#a1">A.1 Shoot the Pictures</a> ·
        <a href="#a2">A.2 Recover Homographies</a> ·
        <a href="#a3">A.3 Warp the Images & Rectification</a> ·
        <a href="#a4">A.4 Blend into a Mosaic</a> ·
        <a href="#a5">A.5 Bells & Whistles</a>
        
      </p>
    </section>

    <hr />

    <!-- ============== OVERVIEW ============== -->
    <section id="overview">
      <h2>Project Overview</h2>
      <div class="prompt">
        <p>
          In this project, we explore image warping and mosaicing — a classic problem in computer vision. 
          The A part of project consists of several steps: first capturing suitable images with overlapping fields of view, then recovering homographies between pairs of images using manually selected correspondences. 
          Next, we perform inverse warping to align images under projective transformations and experiment with rectification to verify correctness. 
          Finally, we blend the warped images into mosaics using weighted averaging or feathering techniques, and optionally extend to <em>cylindrical projection</em> for wide panoramas.
        </p>
        <p>
          For, part B, 
        </p>
  
      </div>
    </section>


    <hr />

    <!-- ============== A1 ============== -->
    <section id="a1">
      <h2>A.1 Shoot the Pictures</h2>
      <div class="prompt">
        <p>
          This set of photos captures the sunset canyon scenery at the classic scenic spot in Yellowstone National Park.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/river1.jpeg" alt="river1 original" />
          <figcaption>left, canyon (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/river2.jpeg" alt="river2 original" />
          <figcaption>medium, canyon (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/river3.jpeg" alt="river3 original" />
          <figcaption>right, canyon (source)</figcaption>
        </figure>
      </div>
      <div class="prompt">
        <p>
          Another set of photos in Yellowstone National Park: the West Thumb Lake.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/lake_left.jpeg" alt="lake1 original" />
          <figcaption>left, lake (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/lake_right.jpeg" alt="lake2 original" />
          <figcaption>right, lake (source)</figcaption>
        </figure>
      </div>
      <div class="prompt">
        <p>
          Another set of photos in Yellowstone National Park: the Norris Basin.
        </p>
      </div>
      <div class="row">
        <figure>
          <img src="./data/basin_left.jpeg" alt="basin original" />
          <figcaption>left, basin (source)</figcaption>
        </figure>
        <figure>
          <img src="./data/basin_right.jpeg" alt="basin2 original" />
          <figcaption>right, basin (source)</figcaption>
        </figure>
      </div>
    </section>

    <hr />

    <!-- ============== A2 ============== -->
    <section id="a2">
      <h2>A.2 Recover Homographies</h2>
      <div class="prompt">
        <p>
            To align two images, we first need to estimate the projective transformation between them, represented by a 3×3 homography matrix. 
    I implemented <code>computeH(im1_pts, im2_pts)</code> based on the normalized Direct Linear Transform (DLT) algorithm. 
    The method takes pairs of manually selected correspondences from two images, normalizes their coordinates to improve numerical stability, 
    constructs an overdetermined linear system <code>Ah = 0</code>, and solves it using singular value decomposition (SVD). 
    The resulting matrix is then denormalized to produce the final homography. 
        </p>
      </div>
<pre><code class="language-python">
def computeH(im1_pts, im2_pts, use_normalization=True):
    im1_pts = np.asarray(im1_pts, dtype=np.float64)
    im2_pts = np.asarray(im2_pts, dtype=np.float64)
    assert im1_pts.shape == im2_pts.shape and im1_pts.shape[0] >= 4
    if use_normalization:
        T1, p1 = normalize_points(im1_pts)
        T2, p2 = normalize_points(im2_pts)
    else:
        T1, p1 = np.eye(3), im1_pts
        T2, p2 = np.eye(3), im2_pts
    A = build_A_normDLT(p1, p2)
    _,_,Vt = np.linalg.svd(A, full_matrices=False)
    Hn = Vt[-1].reshape(3,3)
    H = np.linalg.inv(T2) @ Hn @ T1
    if abs(H[2,2]) > 1e-12: H = H / H[2,2]
    return H
    </code></pre>
      
      <figure>
          <img src="./result/2_correspond.png" alt="corr" width="600" />
          <figcaption>Correspondence Visualization, left and medium</figcaption>
        </figure>
      <!-- A.2 · Text 2 -->
      <p>
      <pre>
      H =
        [[ 1.312538  0.091343 -604.306458]
         [ 0.199424  1.271263 -163.161362]
         [ 0.000249  0.000053   1.000000]]
        mean reprojection error: 1.942 px
      </pre>
      </p>
      
      <div class="answer">
        <p>
        The estimated homography aligns the two river images accurately with an average reprojection error under 2 pixels. 
        The transformation captures both rotation and perspective distortion, as seen from the horizontal shift and slight scaling across the canyon. 
        The correspondences (shown above) match key geometric structures, confirming that the normalization and least-squares SVD approach effectively recovered a stable mapping.
        </p>
      </div>

    </section>

    

    <hr />

    <!-- ============== A3 ============== -->
    <section id="a3">
      <h2>A.3 Warp the Images &amp; Rectification</h2>


      <!-- A.3 · Text 1 + Code -->
      <p>
      I warp images by inverse mapping with a precomputed homography. I first predict the output canvas by transforming the four source corners and build a shift transform so all warped pixels land at nonnegative coordinates. For each output pixel, I map back with H⁻¹ to sample the source. I implement two samplers: nearest neighbor (round to the closest pixel) and bilinear (weighted average of four neighbors). Both return an alpha mask indicating valid samples, which is useful for preview overlays and later blending. 
      </p>

<pre><code class="language-python">
def warpImageNearestNeighbor(im, H, out_shape=None, out_T=None):
    if out_shape is None or out_T is None:
        out_shape, T = make_canvas_and_transform(im.shape, H)
    else:
        T = out_T
    Htot = T @ H
    Hinv = np.linalg.inv(Htot)
    oh, ow = out_shape
    yy, xx = np.meshgrid(np.arange(oh), np.arange(ow), indexing="ij")
    tgt = np.stack([xx, yy, np.ones_like(xx)], axis=-1).reshape(-1,3)
    src = (Hinv @ tgt.T).T
    src_xy = src[:,:2] / src[:,2:3]
    su = np.rint(src_xy[:,0]).astype(int)
    sv = np.rint(src_xy[:,1]).astype(int)
    h, w = im.shape[:2]
    inside = (su&gt;=0)&amp;(su&lt;w)&amp;(sv&gt;=0)&amp;(sv&lt;h)
    out = np.zeros((oh, ow, im.shape[2] if im.ndim==3 else 1), dtype=np.float32)
    if im.ndim==3:
        out = out.reshape(-1,3)
        out[inside] = im[sv[inside], su[inside]]
        out = out.reshape(oh, ow, 3)
    else:
        out = out.reshape(-1,1)
        out[inside,0] = im[sv[inside], su[inside]]
        out = out.reshape(oh, ow)
    alpha = inside.astype(np.float32).reshape(oh, ow)
    return out, alpha, T

def warpImageBilinear(im, H, out_shape=None, out_T=None):
    if out_shape is None or out_T is None:
        out_shape, T = make_canvas_and_transform(im.shape, H)
    else:
        T = out_T
    Htot = T @ H
    Hinv = np.linalg.inv(Htot)
    oh, ow = out_shape
    yy, xx = np.meshgrid(np.arange(oh), np.arange(ow), indexing="ij")
    tgt = np.stack([xx, yy, np.ones_like(xx)], axis=-1).reshape(-1,3)
    src = (Hinv @ tgt.T).T
    uv = src[:,:2] / src[:,2:3]
    u = uv[:,0]; v = uv[:,1]
    x0 = np.floor(u).astype(int); x1 = x0 + 1
    y0 = np.floor(v).astype(int); y1 = y0 + 1
    ax = u - x0; ay = v - y0
    h, w = im.shape[:2]
    valid = (x0&gt;=0)&amp;(x1&lt;w)&amp;(y0&gt;=0)&amp;(y1&lt;h)
    out = np.zeros((oh, ow, im.shape[2] if im.ndim==3 else 1), dtype=np.float32).reshape(-1, (3 if im.ndim==3 else 1))
    if im.ndim==3:
        I00 = np.zeros((tgt.shape[0],3), np.float32); I10 = I00.copy(); I01 = I00.copy(); I11 = I00.copy()
        idx = valid.nonzero()[0]
        if idx.size:
            I00[idx] = im[y0[idx], x0[idx]]
            I10[idx] = im[y0[idx], x1[idx]]
            I01[idx] = im[y1[idx], x0[idx]]
            I11[idx] = im[y1[idx], x1[idx]]
        w00 = (1-ax)*(1-ay); w10 = ax*(1-ay); w01 = (1-ax)*ay; w11 = ax*ay
        out = (I00*w00[:,None] + I10*w10[:,None] + I01*w01[:,None] + I11*w11[:,None])
        out = out.reshape(oh, ow, 3)
    else:
        I00 = np.zeros((tgt.shape[0],1), np.float32); I10 = I00.copy(); I01 = I00.copy(); I11 = I00.copy()
        idx = valid.nonzero()[0]
        if idx.size:
            I00[idx,0] = im[y0[idx], x0[idx]]
            I10[idx,0] = im[y0[idx], x1[idx]]
            I01[idx,0] = im[y1[idx], x0[idx]]
            I11[idx,0] = im[y1[idx], x1[idx]]
        w00 = (1-ax)*(1-ay); w10 = ax*(1-ay); w01 = (1-ax)*ay; w11 = ax*ay
        out = (I00*w00[:,None] + I10*w10[:,None] + I01*w01[:,None] + I11*w11[:,None]).reshape(oh, ow)
    alpha = valid.astype(np.float32).reshape(oh, ow)
    return out, alpha, T
</code></pre>


   <h3>Nearest Neighbor Warping</h3>
      <div class="row" style="align-items: start;">
        <figure style="flex:1;">
          <img src="./result/im1_warp_to_im2_nn.png" alt="im1 warped to im2 using nearest neighbor" style="height:420px; width:auto;" />
          <figcaption>Warped image, nearest neighbor interpolation</figcaption>
        </figure>
        <figure style="flex:1;">
          <img src="./result/3_overlay_nn.png" alt="overlay of nearest neighbor warped image on im2" style="height:420px; width:auto;" />
          <figcaption>Overlay of warped left on medium, NN</figcaption>
        </figure>
      </div>
      
      <h3>Bilinear Warping</h3>
      <div class="row" style="align-items: start;">
        <figure style="flex:1;">
          <img src="./result/im1_warp_to_im2_bilinear.png" alt="im1 warped to im2 using bilinear interpolation" style="height:420px; width:auto;" />
          <figcaption>Warped image, bilinear interpolation</figcaption>
        </figure>
        <figure style="flex:1;">
          <img src="./result/3_overlay_bl.png" alt="overlay of bilinear warped image on im2" style="height:420px; width:auto;" />
          <figcaption>Overlay of warped left on medium, bilinear</figcaption>
        </figure>
      </div>
      
      <div class="answer">
        <p>
          Both warping methods correctly project the first image into the coordinate frame of the second image based on the recovered homography. 
          The nearest neighbor result appears sharp but slightly jagged along edges, as it assigns each target pixel the value of the closest source pixel. 
          In contrast, bilinear interpolation produces smoother transitions and fewer aliasing artifacts, particularly along the canyon ridges and sky boundaries. 
          The overlays show that alignment is consistent and geometry is preserved. 
          The bilinear method offers higher visual quality, while the nearest neighbor version is faster to compute and easier to verify numerically.
        </p>
      </div>

    </section>

    <hr />

    <!-- ============== A4 ============== -->
    <section id="a4">
      <h2>A.4 Blend the Images into a Mosaic</h2>

      <p>
    To create the mosaic, I used the left image and the medium image as the reference pair. 
    The homography computed in the previous step warps <code>left canyon</code> into the coordinate frame of <code>medium canyon</code>. 
    I first generate a shared canvas that fully contains both warped images, then warp <code>left </code> using bilinear interpolation while keeping <code>medium</code> fixed. 
    Each image is assigned a soft alpha mask that decreases toward the borders, and the final mosaic is obtained through weighted averaging of the overlapping regions to achieve smooth blending.
    </p>
      
      <div class="row">
        <figure>
          <img src="./result/mosaic_river12_im1warped.png" alt="mosaic: im1 warped into im2" />
          <figcaption>left, wrapped</figcaption>
        </figure>
        <figure>
          <img src="./result/mosaic_river12_im2oncanvas.png" alt="mosaic: im2 on canvas" />
          <figcaption>meidum, on canvas</figcaption>
        </figure>
      </div>
      <figure>
          <img src="./result/mosaic_river12.png" alt="mosaic blended result" width="600" />
          <figcaption>mosaic, left and medium</figcaption>
        </figure>

      <div class="answer">
        <p>
        The resulting mosaic shows a natural alignment between the canyon walls and the river. 
        The weighted alpha blending effectively removes hard seams and balances brightness across overlapping areas. 
        Some slight ghosting appears where minor geometric differences exist between shots, but overall the transition between <code>river1</code> and <code>river2</code> is smooth and visually coherent. 
        The final output successfully demonstrates projective registration and simple feathering-based compositing.
        </p>
      </div>
        <div class="row">
        <figure>
          <img src="./result/mosaic_lake_left_warped.png" alt="mosaic: im1 warped into im2 l" />
          <figcaption>left, wrapped</figcaption>
        </figure>
        <figure>
          <img src="./result/mosaic_lake_right_oncanvas.png" alt="mosaic: im2 on canvas l" />
          <figcaption>right, on canvas</figcaption>
        </figure>
      </div>
      <figure>
          <img src="./result/mosaic_lake_lr.png" alt="mosaic blended result l" width="800" />
          <figcaption>mosaic, left and right</figcaption>
        </figure>

      <div class="answer">
        <p>
        For the West Thumb Lake, the resulting mosaic is noticeably cleaner and more seamless than the canyon example. 
        This improvement likely comes from the simpler scene geometry — the lake and horizon provide broad, consistent features with fewer sharp depth changes. 
        Because the landscape is more distant and has smoother textures, the alignment and blending steps were easier and produced a clearer final image.
        </p>

      </div>
      
    </section>

    <hr />

    <!-- ============== A5 ============== -->
    <section id="a5">
      <h2>A.5 Bells &amp; Whistles</h2>

      <!-- A.5 · Text 1 -->
<p>
For the final step, I extended the planar mosaicing pipeline to a cylindrical projection to build a wide panorama of canyon left, medium, and right.
Each image is first warped onto a virtual cylinder using inverse mapping and bilinear sampling, where horizontal pixel coordinates are mapped to rotation angles. 
This representation converts camera rotation into horizontal translation, allowing simpler alignment and reducing stretching near the top and bottom edges. 
I then use normalized cross-correlation on the gray and alpha-masked regions to estimate the horizontal offsets between adjacent cylindrical images. 
Finally, I place all warped images on a shared canvas and blend them with soft alpha weights for a seamless transition.
</p>

<p>
A cylindrical projection is particularly suitable here because the camera mainly rotated horizontally while capturing the canyon. 
It preserves straight vertical structures and prevents extreme distortion that would appear in a planar projection. 
In contrast, spherical mapping would be necessary only if the capture involved significant up–down rotation. 
</p>

<p> 
The focal length in pixels was derived from the EXIF data of the iPhone 16 Pro main camera (24 mm lens, 4284 px width, 13.4 mm sensor width): 
<span>f<sub>px</sub> ≈ 24 × (4284 / 13.4) ≈ 7700 px</span>.
And all three photos were taken with the same focal length.
</p>

      <div class="wide">
        <figure>
          <img src="./result/cylindrical_panorama_river123.png" alt="cylindrical panorama river123" width="700" />
          <figcaption>cylindrical panorama of canyon, left/medium/right </figcaption>
        </figure>
      </div>

<div class="answer">
  <p>
  The cylindrical panorama successfully combines all three views into a continuous wide-angle image. 
  The curved projection removes the perspective stretching that would occur in a planar mosaic and produces a natural sense of depth across the canyon. 
  Vertical structures remain nearly straight, confirming that the cylindrical model fits the mostly horizontal camera motion well. 
</p>

<p> 
  However, compared with the two-image mosaic in Part A.4, the alignment appears slightly less precise near the central ridge. 
  This small mismatch likely comes from imperfect offset estimation in normalized cross-correlation, as cylindrical warping changes local geometry and slightly reduces texture consistency across the overlap. 
  Despite that, the overall transition is smooth, and the result illustrates the benefit of cylindrical projection for wide panoramas.
  </p>
</div>

    </section>

    <hr />

<!-- ============== B1 ============== -->
<section id="b1">
  <h2>B.1 Harris Corner Detection</h2>

  <div class="prompt">
    <p>
      I detect corners using the Harris response at a single scale and visualize all peaks above a threshold. 
      Then I apply Adaptive Non-Maximal Suppression (ANMS) to select spatially well-distributed, high-quality corners.
    </p>
  </div>

  <h3>Harris corners (all candidates)</h3>
  <div class="row">
    <figure>
      <img src="./result/B1/river_left_harris_all.jpg" alt="left image, Harris corners (all)" />
      <figcaption>left, Harris corners (all)</figcaption>
    </figure>
    <figure>
      <img src="./result/B1/river_right_harris_all.jpg" alt="right image, Harris corners (all)" />
      <figcaption>right, Harris corners (all)</figcaption>
    </figure>
  </div>

  <h3>ANMS-selected corners</h3>
  <div class="row">
    <figure>
      <img src="./result/B1/river_left_anms_500.jpg" alt="left image, ANMS 500" />
      <figcaption>left, ANMS (top 500)</figcaption>
    </figure>
    <figure>
      <img src="./result/B1/river_right_anms_500.jpg" alt="right image, ANMS 500" />
      <figcaption>right, ANMS (top 500)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      ANMS removes clusters of nearby responses and preserves coverage across the frame, which leads to more stable matching later.
    </p>
  </div>
</section>

<hr />

<!-- ============== B2 ============== -->
<section id="b2">
  <h2>B.2 Feature Descriptor Extraction</h2>

  <div class="prompt">
    <p>
      Around each selected corner I extract an axis-aligned 40×40 window, blur and subsample to an 8×8 patch, then bias/gain-normalize to zero mean and unit variance to form the descriptor.
    </p>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B2/river_left_desc_montage.jpg" alt="left descriptors montage" />
      <figcaption>left, descriptor montage (normalized 8×8 samples)</figcaption>
    </figure>
    <figure>
      <img src="./result/B2/river_right_desc_montage.jpg" alt="right descriptors montage" />
      <figcaption>right, descriptor montage (normalized 8×8 samples)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      The blurred 40→8 subsampling reduces noise and increases descriptor stability while keeping computation light.
    </p>
  </div>
</section>

<hr />

<!-- ============== B3 ============== -->
<section id="b3">
  <h2>B.3 Feature Matching</h2>

  <div class="prompt">
    <p>
      I match descriptors with nearest-neighbor search and apply Lowe’s ratio test between the first and second nearest distances to reject ambiguous matches.
    </p>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B3/river_LR_matches_top50.jpg" alt="top-50 raw matches" />
      <figcaption>top-50 nearest matches (visualization)</figcaption>
    </figure>
    <figure>
      <img src="./result/B3/river_LR_matches_ratio70.jpg" alt="matches after ratio test 0.70" />
      <figcaption>matches after ratio test (τ = 0.70)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      The ratio filter removes many false positives along repetitive textures, leaving a cleaner inlier pool for robust model fitting.
    </p>
  </div>
</section>

<hr />

<!-- ============== B4 ============== -->
<section id="b4">
  <h2>B.4 RANSAC for Robust Homography and Auto Mosaics</h2>

  <div class="prompt">
    <p>
      I run 4-point RANSAC on the matched pairs to estimate a projective homography, scoring each hypothesis by reprojection error. 
      The best model is used to warp and blend images with the Part A pipeline, producing automatic mosaics.
    </p>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B4/mosaic_river12_auto.jpg" alt="auto mosaic river (left+medium)" />
      <figcaption>auto mosaic: canyon (left + medium)</figcaption>
    </figure>
    <figure>
      <img src="./result/B4/mosaic_lake_auto.jpg" alt="auto mosaic lake" />
      <figcaption>auto mosaic: lake (left + right)</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B4/mosaic_basin_auto.jpg" alt="auto mosaic basin" />
      <figcaption>auto mosaic: basin (left + right)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      Across scenes, RANSAC eliminates outliers and yields homographies close to the manual version. 
      The lake and basin panoramas appear especially clean due to simpler geometry and larger viewing distance.
    </p>
  </div>
</section>

<hr />

<!-- ============== B5 ============== -->
<section id="b5">
  <h2>B.5 Bells &amp; Whistles</h2>

  <div class="prompt">
    <p>
      I add multiscale processing: corners are detected over multiple smoothing levels and descriptors are formed per scale. 
      This improves feature coverage across details of different sizes and increases match robustness.
    </p>
  </div>

  <h3>Multiscale examples and overlays</h3>
  <div class="row">
    <figure>
      <img src="./result/B5/river1_multiscale_montage.jpg" alt="river1 multiscale montage" />
      <figcaption>river1 multiscale responses / selections</figcaption>
    </figure>
    <figure>
      <img src="./result/B5/river1_multiscale_overlay.jpg" alt="river1 multiscale overlay" />
      <figcaption>river1 multiscale overlay on image</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B5/river2_multiscale_montage.jpg" alt="river2 multiscale montage" />
      <figcaption>river2 multiscale responses / selections</figcaption>
    </figure>
    <figure>
      <img src="./result/B5/river2_multiscale_overlay.jpg" alt="river2 multiscale overlay" />
      <figcaption>river2 multiscale overlay on image</figcaption>
    </figure>
  </div>

  <div class="row">
    <figure>
      <img src="./result/B5/river_multi.jpg" alt="multi-image panorama (bonus)" />
      <figcaption>multi-image panorama (autostitched)</figcaption>
    </figure>
  </div>

  <div class="answer">
    <p>
      Multiscale detection supplies more distinctive features on both coarse structures and fine textures, reducing failure cases in low-texture areas and improving overall stitch quality.
    </p>
  </div>
</section>


  </main>

  <footer>
    <p class="muted">© <span id="y"></span> · CS180/280A · <a href="../index.html">Home</a></p>
  </footer>

  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>

