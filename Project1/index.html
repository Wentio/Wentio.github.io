<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>CS280A · Project 1 — Colorizing the Prokudin-Gorskii Collection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --max: 860px; }
    * { box-sizing: border-box; }
    body { margin: 24px; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif; line-height: 1.6; background:#fafafa;}
    main, header, footer { max-width: var(--max); margin: 0 auto; }
    h1, h2, h3 { line-height: 1.25; }
    .muted { color:#666; font-size: 14px; }
    .pair { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 14px; margin:14px 0;}
    figure { margin: 0; border: 1px solid #eaeaea; border-radius: 10px; padding: 10px; background: #fff; }
    figcaption { margin-top: 6px; font-size: 14px; color: #555; }
    img { max-width: 100%; height: auto; display: block; background: #f5f5f5; border-radius: 6px; }
    .answer { border-left: 4px solid #2f6feb; background: #f0f6ff; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    .prompt { border-left: 4px solid #999; background: #f7f7f7; padding: 12px 14px; border-radius: 8px; margin:12px 0;}
    .code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; }
    nav a { text-decoration: none; }
    hr { border: none; height: 1px; background: #eee; margin: 28px 0; }
    .toc a { color:#2f6feb; text-decoration:none; }
  </style>
</head>
<body>
  <header>
    <p class="muted"><a href="../index.html">← Back to Home</a></p>
    <h1>Project 1 — Colorizing the Prokudin-Gorskii Collection</h1>
    <p class="muted">CS280A · Wentio Li · Sept 2025</p>
  </header>

  <main>
    <!-- TOC -->
    <section class="toc">
      <p>
        <a href="#overview">1. Overview</a> ·
        <a href="#basic">2. Basic Alignment</a> ·
        <a href="#bw">3. Bells &amp; Whistles</a> ·
        <a href="#params">4. Parameters</a>
      </p>
    </section>

    <hr />

    <!-- 1. Project Overview -->
    <section id="overview">
      <h2>1. Project Overview</h2>
      <div class="prompt">
        <p>
          In this project, I implemented an end-to-end pipeline to reconstruct color images from the Prokudin-Gorskii glass plate negatives. Each input consists of a single grayscale image with three vertically stacked channels in the order <b>B–G–R</b>. The task is to split these channels, align them by translation, and recombine them into a single RGB output with minimal artifacts.
        </p>
        <p>
          My pipeline starts with a <b>single-scale exhaustive search</b> for low-resolution images, where the green and red channels are shifted relative to blue within a fixed displacement window. Alignment quality is evaluated using <b>Normalized Cross-Correlation (NCC)</b> as the similarity metric. To handle large displacements in high-resolution scans, I extended this to a <b>coarse-to-fine pyramid search</b>: images are recursively downsampled, aligned at a coarse level, and progressively refined at higher resolutions.
        </p>
        <p>
          For the CS280A requirements, I incorporated several enhancements to improve the visual quality beyond alignment. These include <b>automatic cropping</b> of inconsistent borders, <b>contrast stretching</b> to expand dynamic range, <b>white balance correction</b> using white-patch and gray-world assumptions, a learned <b>3×3 color correction matrix</b> to better map raw channels to display RGB, and <b>gradient-based features</b> for more robust alignment across channels with brightness differences.
        </p>
        <p>
          Together, these steps form a robust and efficient pipeline capable of producing high-quality color reconstructions for the images from the LoC Prokudin-Gorskii collection.
        </p>
      </div>
      <div class="pair">
        <figure><img src="./media/raw.jpg" alt="overview" /><figcaption>cathedral, raw</figcaption></figure>
      </div>
      <div class="answer">Above is the raw BGR image from the raw image. From the negatives, we can observe that the three images differ slightly in angle, edges, and tilt. These discrepancies undoubtedly present challenges for the next step of alignment. </div>
    </section>

    <hr />

    <!-- 2. Basic Alignment -->
    <section id="basic">
      <h2>2. Basic Alignment</h2>

      <!-- 2.1 Single-scale -->
      <section id="basic-single">
        <h3>2.1 Single-Scale Alignment</h3>
        <div class="prompt">
          <p>
            For the low-resolution images, the green and red channels are shifted relative to the blue reference. 
            Each candidate displacement is evaluated within a fixed search window of <code>[−15, +15]</code> pixels using <b>Normalized Cross-Correlation (NCC)</b> as the primary metric, 
            with <b>L2/SSD</b> also tested for comparison. To reduce the influence of misaligned borders and strong color fringes, the score is computed only on an interior crop 
            (5% removed from each side). 
          </p>
          <p>
            The search proceeds by rolling the moving channel with <code>np.roll</code>, scoring every offset, and selecting the best match. 
            This brute-force approach is computationally simple but sufficient for small plates such as <i>cathedral</i>, <i>monastery</i>, and <i>tobolsk</i>. 
            The output is then composed as <code>[R<sub>aligned</sub>, G<sub>aligned</sub>, B]</code>, clipped to <code>[0,1]</code>, with displacement vectors, similarity scores, 
            and runtime recorded for analysis.
          </p>
        </div>

        <div class="pair">
          <figure>
            <img src="./media/main/single/cathedral_single_ncc.jpg" alt="cathedral single" />
            <figcaption>
              cathedral, single, time=0.27s<br />
              G(2, 5), R(3, 12)
            </figcaption>
          </figure>
          <figure>
            <img src="./media/main/single/monastery_single_ncc.jpg" alt="monastery single" />
            <figcaption>
              monastery, single, time=0.28s<br />
              G(2, −3), R(2, 3)
            </figcaption>
          </figure>
          <figure>
            <img src="./media/main/single/tobolsk_single_ncc.jpg" alt="tobolsk single" />
            <figcaption>
              tobolsk, single, time=0.26s<br />
              G(2, 3), R(3, 6)
            </figcaption>
          </figure>
        </div>
        <div class="answer">
        <p>
          The single-scale exhaustive search with NCC successfully aligned the low-resolution images. 
          Runtime was efficient, averaging about <b>0.27s per image</b>.
          For <i>cathedral</i>, the offsets were <code>G(2, 5)</code> and <code>R(3, 12)</code>, 
          producing a clean result though with some residual colored borders. 
          <i>Monastery</i> and <i>Tobolsk</i> required smaller displacements, leading to strong vertical alignment in the picture. 
        </p>
      
        <p>
          Visually, all three reconstructions demonstrate good channel registration in the central regions, 
          but noticeable color fringing remains along the borders. This is expected since border regions 
          are more sensitive to cropping, tilt, and exposure differences across the three negatives. 
      
        </p>
        </div>

      </section>

      <!-- 2.2 Pyramid -->
      <section id="basic-pyramid">
        <h3>2.2 Pyramid (Multi-Stage) Alignment</h3>
        <div class="prompt">
          <p>
            Starting from the coarsest scale (constructed by repeatedly downsampling by ×0.5 until the min side is ≈ <code>pyr_min_side=300</code>), I align GRB with a limited window and then propagate the estimated offset to the next finer level. At each scale, the search is centered at the up-scaled previous estimate and restricted to a small local window (<code>top_win=12</code> at the coarsest level, then <code>local_win=6</code> per level). Candidate shifts are applied with <code>np.roll</code>, and similarity is evaluated using NCC by default (with L2/SSD available for ablation). Alike Single-Scale Alignment, scores are computed on an interior crop with <code>score_inner_crop_ratio=0.05</code>.
          </p>
          <p>
            This strategy reduces the search complexity from a large global window at full resolution to a sequence of small local refinements, making alignment both faster and more stable> under large displacements, mild tilt, and exposure differences.
          </p>
        </div>
        <div class="pair">
          <figure>
            <img src="./media/main/cathedral_pyr_ncc.jpg" alt="cathedral pyramid" />
            <figcaption>
              cathedral, pyramid, time=0.13s<br />
              G(2, 5), R(3, 12)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/monastery_pyr_ncc.jpg" alt="monastery pyramid" />
            <figcaption>
              monastery, pyramid, time=0.11s<br />
              G(2, -3), R(2, 3)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/tobolsk_pyr_ncc.jpg" alt="tobolsk pyramid" />
            <figcaption>
              tobolsk, pyramid, time=0.13s<br />
              G(2, 3), R(3, 6)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/church_pyr_ncc.jpg" alt="church pyramid" />
            <figcaption>
              church, pyramid, time=69.85s<br />
              G(3, 25), R(-5, 58)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/emir_pyr_ncc.jpg" alt="emir pyramid" />
            <figcaption>
              emir, pyramid, time=90.08s<br />
              G(24, 49), R(43, 103)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/harvesters_pyr_ncc.jpg" alt="harvesters pyramid" />
            <figcaption>
              harvesters, pyramid, time=90.05s<br />
              G(16, 60), R(14, 124)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/icon_pyr_ncc.jpg" alt="icon pyramid" />
            <figcaption>
              icon, pyramid, time=87.79s<br />
              G(17, 40), R(23, 89)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/italil_pyr_ncc.jpg" alt="italil pyramid" />
            <figcaption>
              italil, pyramid, time=79.83s<br />
              G(21, 38), R(35, 77)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/lastochikino_pyr_ncc.jpg" alt="lastochikino pyramid" />
            <figcaption>
              lastochikino, pyramid, time=82.90s<br />
              G(-2, -3), R(-9, 76)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/lugano_pyr_ncc.jpg" alt="lugano pyramid" />
            <figcaption>
              lugano, pyramid, time=83.06s<br />
              G(-17, 41), R(-28, 92)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/melons_pyr_ncc.jpg" alt="melons pyramid" />
            <figcaption>
              melons, pyramid, time=77.56s<br />
              G(9, 82), R(11, 177)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/self_portrait_pyr_ncc.jpg" alt="self portrait pyramid" />
            <figcaption>
              self_portrait, pyramid, time=59.37s<br />
              G(29, 79), R(34, 175)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/siren_pyr_ncc.jpg" alt="siren pyramid" />
            <figcaption>
              siren, pyramid, time=80.09s<br />
              G(-7, 49), R(-25, 96)
            </figcaption>
          </figure>
        
          <figure>
            <img src="./media/main/three_generations_pyr_ncc.jpg" alt="three generations pyramid" />
            <figcaption>
              three_generations, pyramid, time=79.50s<br />
              G(13, 55), R(10, 112)
            </figcaption>
          </figure>
        </div>

        <div class="answer">
          <p>
            The pyramid-based alignment with NCC achieved consistent registration across ] fourteen test images. 
            For the three smaller JPGs, the results were obtained very quickly 
            (around 0.1s per image) with only small displacements, showing that the method is both efficient and reliable 
            when only modest shifts are present.
          </p>
        
          <p>
            For the larger TIFFs, displacements were often much larger (tens of pixels in both directions), and runtimes 
            increased to about 60–90 seconds depending on the image. Despite these challenges, the pyramid strategy was able 
            to recover stable alignments in all cases, as seen in church, emir, harvesters, and other examples. 
            This demonstrates the effectiveness of the coarse-to-fine approach in handling large-scale translations that 
            would be impractical for a single-scale search.
          </p>
        
          <p>
            Visually, the reconstructed images show stable alignment in central regions and good structural consistency across channels. 
            Some border artifacts and residual color fringes remain, particularly in images with extreme shifts or geometric distortions, 
            but overall the pyramid alignment provides a solid foundation for later post-processing.
          </p>
        </div>
      </section>
    </section>

    <hr />

    <!-- 3. Bells & Whistles -->
    <section id="bw">
      <h2>3. Bells &amp; Whistles</h2>

      <!-- 3.1 Cropping -->
      <section id="bw-crop">
        <h3>3.1 Automatic Cropping</h3>
        <div class="prompt">
          <p>
            Our goal is to remove the distracting borders that remain after alignment by detecting channel inconsistency along the image edges. 
            Instead of trimming a fixed margin, the algorithm computes the per-pixel standard deviation across channels 
            (σ map = √Var(R,G,B)) and projects it onto rows and columns to form 1D signals. These signals are smoothed with a moving average 
            filter of width <code>smooth</code>.
          </p>
          <p>
            In the central region, we compute the median response as a baseline (<i>base</i>), and then define thresholds as 
            <code>thr_high = base × k_high</code> and <code>thr_low = base × k_low</code>. 
            Values above <i>thr_high</i> usually correspond to strong color fringes at the borders, while values below <i>thr_low</i> 
            indicate empty or dark margins. Removing both ensures that only the stable content region is kept. 
            The crop box is then determined adaptively as (left, top, right, bottom). An optional 
            <code>max_trim_ratio</code> can be provided to prevent excessive cropping (e.g., no more than 10% of the height or width).
            In practice, we used <code>k_high≈1.25</code>, <code>k_low≈1.05</code>, and <code>smooth≈7</code>.
          </p>
        </div>
        <div class="pair">
          <figure><img src="./media/main/italil_pyr_ncc.jpg" alt="crop before" /><figcaption>italil, before.</figcaption></figure>
          <figure><img src="./media/BW/crop/italil_pyr_ncc_crop.jpg" alt="crop after" /><figcaption>italil, after.</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            The automatic cropping step was effective at removing most of the colored borders introduced during channel alignment. 
            In the italil example, the bright red and blue fringes at the top and bottom were successfully detected and trimmed, 
            leading to a cleaner and more natural appearance. 
            However, some residual borders remain — in this case, the left edge was not completely removed. 
            This indicates that while the method captures the most obvious inconsistencies, it can occasionally miss weaker border artifacts 
            if their intensity is closer to the valid image regions. 
          </p>
        </div>
      </section>

      <!-- 3.2 Contrast -->
      <section id="bw-contrast">
        <h3>3.2 Automatic Contrast</h3>
        <div class="prompt">
        <p>
          The automatic contrast adjustment first rescales pixel intensities 
          based on percentile stretching and then applies an <b>S-curve (sigmoid) mapping</b>. The percentile stretch step 
          normalizes each channel to [0, 1] by clipping extreme values at the low and high quantiles.
        </p>
        <p>
          After normalization, a sigmoid mapping is applied:
          <code>sigmoid(x) = 1 / (1 + exp(-gain · (x − cutoff)))</code>. 
          Here, <b>gain</b> controls the steepness of the curve — higher gain values produce stronger contrast, especially 
          around the midtones — while <b>cutoff</b> determines the midpoint that is mapped to 0.5. 
          In practice, I used settings around <code>gain=9</code> and <code>cutoff=0.45</code>, which produced balanced 
          enhancement without overshooting. Compared to traditional gamma correction, which often fails to sufficiently 
          enhance midtone structure in these historical images, the sigmoid S-curve provides stronger and more natural-looking results.
        </p>
      </div>
         <div class="pair">
          <figure><img src="./media/main/italil_pyr_ncc.jpg" alt="contrast before" /><figcaption>italil, before.</figcaption></figure>
          <figure><img src="./media/BW/contrast/italil_pyr_ncc_contrast.jpg" alt="contrast after" /><figcaption>italil, after.</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            The automatic contrast adjustment significantly improves the visual clarity of the image. 
            In this case, midtones become more vivid, and darker regions such as the subject’s clothing reveal more detail, 
            while highlights like the background appear brighter. 
          </p>
        </div>
      </section>

      <!-- 3.3 White Balance -->
      <section id="bw-wb">
        <h3>3.3 Automatic White Balance</h3>        
        <div class="prompt">
          <p>
            The goal of white balance is to neutralize the scene illuminant so that achromatic surfaces appear gray.
            I estimate per-channel gains <i>g</i> = (g<sub>R</sub>, g<sub>G</sub>, g<sub>B</sub>) and rescale the image
            by channel-wise multiplication, clipping results to [0,1] to avoid overflow.
          </p>
        
          <p>
            <b>Gray-World.</b> Assume the average scene color should be neutral.
            Let μ<sub>c</sub> be the mean intensity of channel <i>c</i> and define the overall mean
            <span style="text-decoration:overline;">μ</span> = (μ<sub>R</sub> + μ<sub>G</sub> + μ<sub>B</sub>)/3.
            The gains are g<sub>c</sub> = <span style="text-decoration:overline;">μ</span> / μ<sub>c</sub>
            (with a small ε for stability) and then clipped to <code>[1/max_gain, max_gain]</code>.
            In practice I used <code>max_gain ≈ 3.0</code> to correct color casts while avoiding over-amplification.
          </p>
        
          <p>
            <b>Shades-of-Gray.</b> To make the estimator less sensitive to dark regions and closer to white-patch behavior,
            use the L<sup>p</sup> mean:
            m<sub>p,c</sub> = ( E[ I<sub>c</sub><sup>p</sup> ] )<sup>1/p</sup>,
            and the overall mean
            <span style="text-decoration:overline;">m</span><sub>p</sub> =
            ( m<sub>p,R</sub> + m<sub>p,G</sub> + m<sub>p,B</sub> ) / 3.
            Gains are g<sub>c</sub> = <span style="text-decoration:overline;">m</span><sub>p</sub> / m<sub>p,c</sub>,
            again clipped by <code>max_gain</code>.
            Here <i>p</i> controls aggressiveness: <i>p</i>=1 recovers Gray-World, while large <i>p</i> approaches White-Patch.
            I found <code>p ≈ 6</code> and <code>max_gain ∈ [3, 4]</code> give balanced corrections on these plates.
          </p>
        </div>

        <div class="pair">
          <figure><img src="./media/main/church_pyr_ncc.jpg" alt="wb before" /><figcaption>church, before.</figcaption></figure>
          <figure><img src="./media/BW/whiteb/church_pyr_ncc_wb_shades_of_gray.jpg" alt="wb after" /><figcaption>church, after, Shades-of-Gray</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            After exploration, we find Shades-of-Gray methods shows better outcome. It removes the strong blue cast that is visible in the original image. 
            In the adjusted result, the church walls appear closer to neutral white and the overall scene regains a more natural color tone. 
            The sky and water also show improved balance, shifting from an unnatural purple-blue toward more realistic shades. 
          </p>
        </div>
      </section>

      <!-- 3.4 Color Mapping -->
      <section id="bw-colormap">
        <h3>3.4 Better Color Mapping</h3>
        <div class="prompt">
          <p>
            To improve color rendition beyond white balance, I estimate a 3×3 color correction matrix (CCM) that maps the raw
            channels to display RGB. Training pixels are selected automatically: luminance is computed as <code>Y = 0.299R + 0.587G + 0.114B</code>, and only midtones are kept
            using quantiles <code>[mid_lo, mid_hi]</code>. Among these, near-neutral pixels are favored by requiring
            <code>range = max(R,G,B) − min(R,G,B) &lt; neutral_tau</code>; if too few remain, the selection gracefully falls back
            to midtones or all pixels.
          </p>
          <p>
            Let the selected RGBs form <code>A ∈ ℝ^{N×3}</code>. Each row is encouraged to map toward its own gray value, so the
            target is <code>Y ∈ ℝ^{N×3}</code> with <code>Y_i = mean(A_i) · [1,1,1]</code>. The CCM <code>M ∈ ℝ^{3×3}</code> is
            obtained by a ridge-regularized least squares:
            <code>M = argmin_M ‖A M − Y‖² + λ‖M − I‖²</code>, implemented as
            <code>M = (AᵀA + λI)<sup>−1</sup>(AᵀY + λI)</code> with <code>lambda_reg = λ</code>. Rows are then normalized to sum
            to 1 (brightness preservation), off-diagonals are softly shrunk toward identity
            (<code>M ← I + shrink_offdiag · (M − I)</code>) to limit cross-talk, and the final matrix is blended with identity
            (<code>M ← (1 − blend)·I + blend·M</code>) for stability.
          </p>
          <p>
            The mapped image is <code>RGB_out = clip(RGB_in · M, 0, 1)</code>. A mild saturation boost in HSV space
            (<code>gain</code> around 1.05–1.10) can be applied as a cosmetic step. Typical settings used:
            <code>border=0.06</code>, <code>mid_lo=0.08</code>, <code>mid_hi=0.92</code>, <code>neutral_tau=0.06–0.07</code>,
            <code>lambda_reg=0.03–0.05</code>, <code>shrink_offdiag=0.6</code>, <code>blend=0.25</code>, saturation
            <code>gain≈1.08</code>.
          </p>
        </div>
        <div class="pair">
          <figure><img src="./media/main/church_pyr_ncc.jpg" alt="colormap before" /><figcaption>church, before.</figcaption></figure>
          <figure><img src="./media/BW/color/church_pyr_ncc_ccm.jpg" alt="colormap after" /><figcaption>church, after.</figcaption></figure>
        </div>
        <div class="answer">
          <p>
            Applying the learned color correction matrix produced only a modest improvement for the church example. 
            The overall color cast is reduced slightly and the tones are warmer than before, but the difference is subtle 
            compared to the original white balance adjustment. This is consistent with the fitted CCM, which is very close 
            to the identity matrix 
            (<code>[[0.90, 0.05, 0.05], [0.05, 0.90, 0.05], [0.05, 0.05, 0.90]]</code>), 
            indicating that the selected neutral pixels provided only a weak correction signal. 
            As the unnatural purple-blue still remains, stronger effects would require either a richer selection of training samples or other methods.
          </p>
        </div>
      </section>

      <!-- 3.5 Better Features -->
      <section id="bw-features">
        <h3>3.5 Better Features</h3>
        <div class="prompt">
          <p>
            I replace raw intensities with a <i>feature map</i> before scoring. 
            Three modes are supported: <code>raw</code> (identity), <code>grad</code> (Sobel gradient magnitude), and <code>edge</code> (Canny edges). 
            For the gradient mode, horizontal/vertical Sobel filters (<code>kx</code>, <code>ky</code>) are convolved with each channel image to obtain 
            <code>gx</code> and <code>gy</code>, then the magnitude <code>mag = sqrt(gx^2 + gy^2)</code> is computed. The resulting feature map is standardized 
            to zero mean and unit variance to stabilize the similarity score across images. This emphasizes structure (edges, corners, texture) rather than absolute brightness.
          </p>
          <p>
            Alignment then proceeds exactly as in the baseline: the moving channel is shifted within the search window and scored by NCC (or L2), 
            but the score is computed on the feature maps instead of raw pixels. In the single-scale version this is 
            <code>exhaust_align_feature(..., win=15, crop_ratio=0.05, metric="ncc", feat="grad")</code>. 
            In the pyramid version, the current best offset is propagated to the next level and refined locally 
            (<code>top_win=12</code>, <code>local_win=6</code>), again evaluating NCC on features with an interior crop to suppress border artifacts.
          </p>
          <p>
            Typical settings: <code>feat="grad"</code>, <code>metric="ncc"</code>, <code>crop_ratio=0.05</code>, 
            single-scale window <code>±15</code> px; for the pyramid, <code>pyr_min_side≈300</code>, <code>top_win=12</code>, <code>local_win=6</code>. 
          </p>
        </div>

        <div class="pair">
          <figure><img src="./media/main/self_portrait_pyr_ncc.jpg" alt="raw features" /><figcaption>self_portrait, raw matching.</figcaption></figure>
          <figure><img src="./media/BW/feature/self_portrait_pyr_ncc_grad.jpg" alt="gradient features" /><figcaption>self_portrait, gradient-based matching.</figcaption></figure>
        </div>
        <div class="pair">
          <figure><img src="./media/BW/feature/before.jpg" alt="raw features 1" /><figcaption>self_portrait, raw matching, enlarged.</figcaption></figure>
          <figure><img src="./media/BW/feature/after.jpg" alt="gradient features 1" /><figcaption>self_portrait, gradient-based matching, enlarged.</figcaption></figure>
        </div>
        <div class="answer">Analysis of feature-based alignment.</div>
      </section>
    </section>

    <hr />


    <!-- 4. Parameters -->
    <section id="params">
      <h2>4. Parameters &amp; Environment</h2>
      <ul>
        <li>Search window (single-scale): ±15 px</li>
        <li>Pyramid: scale ×0.5, top-level ±12 px, per-level ±5–7 px</li>
        <li>Feature: Sobel gradient magnitude</li>
        <li>Metric: ZNCC</li>
        <li>Contrast: percentile stretch 1%–99%</li>
        <li>White balance: white-patch + gray-world</li>
        <li>Color mapping: 3×3 fixed matrix (values here)</li>
        <li>Runtime: &lt;1 min per image</li>
        <li>Software: Python, NumPy, OpenCV</li>
      </ul>
    </section>

  </main>

  <footer>
    <p class="muted">© <span id="y"></span> · CS280A · <a href="../index.html">Home</a></p>
  </footer>

  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>
